\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\abx@aux@refcontext{nyt/global//global/global/global}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\abx@aux@cite{0}{allier_decomposing_2024}
\abx@aux@segm{0}{0}{allier_decomposing_2024}
\abx@aux@cite{0}{mi_connectome-constrained_2021}
\abx@aux@segm{0}{0}{mi_connectome-constrained_2021}
\abx@aux@cite{0}{pospisil_fly_2024}
\abx@aux@segm{0}{0}{pospisil_fly_2024}
\abx@aux@cite{0}{dorkenwald_neuronal_2024}
\abx@aux@segm{0}{0}{dorkenwald_neuronal_2024}
\abx@aux@cite{0}{wang_foundation_2025}
\abx@aux@segm{0}{0}{wang_foundation_2025}
\babel@aux{english}{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The temporal activity of a simulated neural network (\textbf  {a}) is converted into densely connected graph (\textbf  {b}) processed by a message passing GNN (\textbf  {c}). Each neuron (node $i$) receives activity signals $x_j$ from connected neurons (node $j$), processed by a transfer function $\psi ^*$ and weighted by the matrix $\ensuremath  {\boldsymbol  {W}}$. The sum of these messages is updated with functions $\phi ^*$ and $\Omega ^*$ to obtain the predicted activity rate $\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \dot  {\ensuremath  {\boldsymbol  {x}}}$}\mathaccent "0362{\dot  {\ensuremath  {\boldsymbol  {x}}}}_{i}$. In addition to the observed activity $x_i$, the GNN has access to learnable latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ associated with each node $i$.}}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GNN structure}{{1}{1}{The temporal activity of a simulated neural network (\textbf {a}) is converted into densely connected graph (\textbf {b}) processed by a message passing GNN (\textbf {c}). Each neuron (node $i$) receives activity signals $x_j$ from connected neurons (node $j$), processed by a transfer function $\psi ^*$ and weighted by the matrix $\Mat {W}$. The sum of these messages is updated with functions $\phi ^*$ and $\Omega ^*$ to obtain the predicted activity rate $\widehat {\dot {\Vec {x}}}_{i}$. In addition to the observed activity $x_i$, the GNN has access to learnable latent vectors $\Vec {a}_i$ associated with each node $i$}{figure.caption.1}{}}
\newlabel{fig:GNN structure@cref}{{[figure][1][]1}{[1][1][]1}{}{}{}}
\abx@aux@cite{0}{stern_reservoir_2023}
\abx@aux@segm{0}{0}{stern_reservoir_2023}
\abx@aux@cite{0}{stern_dynamics_2014}
\abx@aux@segm{0}{0}{stern_dynamics_2014}
\abx@aux@cite{0}{sitzmann_implicit_2020}
\abx@aux@segm{0}{0}{sitzmann_implicit_2020}
\abx@aux@cite{0}{fey_fast_2019}
\abx@aux@segm{0}{0}{fey_fast_2019}
\abx@aux@cite{0}{allier_decomposing_2024}
\abx@aux@segm{0}{0}{allier_decomposing_2024}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}\protected@file@percent }
\newlabel{eqn:simulation}{{1}{2}{Simulation of neural assemblies}{equation.1}{}}
\newlabel{eqn:simulation@cref}{{[equation][1][]1}{[1][2][]2}{}{}{}}
\newlabel{eqn:simulation2}{{2}{2}{Simulation of neural assemblies}{equation.2}{}}
\newlabel{eqn:simulation2@cref}{{[equation][2][]2}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Graph neural networks}{2}{subsection.2.1}\protected@file@percent }
\newlabel{eqn:GNN}{{2.1}{2}{Graph neural networks}{subsection.2.1}{}}
\newlabel{eqn:GNN@cref}{{[subsection][1][2]2.1}{[1][2][]2}{}{}{}}
\newlabel{eqn:loss_prediction}{{2.1}{2}{Graph neural networks}{subsection.2.1}{}}
\newlabel{eqn:loss_prediction@cref}{{[subsection][1][2]2.1}{[1][2][]2}{}{}{}}
\abx@aux@cite{0}{cranmer_interpretable_2023}
\abx@aux@segm{0}{0}{cranmer_interpretable_2023}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \numprint  {1000} densely connected neurons with 4 neuron-dependent update functions. (\textbf  {a}) Activity time series used for GNN training. This dataset contains $10^5$ time-points. (\textbf  {b}) Sample time series taken from (\textbf  {a}). (\textbf  {c}) True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. The inset shows $20\times 20$ weights. (\textbf  {d})~Learned connectivity. (\textbf  {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref  {eqn:simulation}). (\textbf  {f}) Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ of all neurons. Colors correspond to different neuron types. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a_i}}, x_i)$. The plot shows 1000 overlaid curves, one for each vector $\ensuremath  {\boldsymbol  {a_i}}$. True functions are overlaid in light gray. (\textbf  {h}) Learned transfer function $\psi ^*(x_i)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig2}{{2}{3}{\np {1000} densely connected neurons with 4 neuron-dependent update functions. (\textbf {a}) Activity time series used for GNN training. This dataset contains $10^5$ time-points. (\textbf {b}) Sample time series taken from (\textbf {a}). (\textbf {c}) True connectivity $\Mat {W}_{ij}$. The inset shows $20\times 20$ weights. (\textbf {d})~Learned connectivity. (\textbf {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref {eqn:simulation}). (\textbf {f}) Learned latent vectors $\Vec {a}_i$ of all neurons. Colors correspond to different neuron types. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a_i}, x_i)$. The plot shows 1000 overlaid curves, one for each vector $\Vec {a_i}$. True functions are overlaid in light gray. (\textbf {h}) Learned transfer function $\psi ^*(x_i)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.3}{}}
\newlabel{fig2@cref}{{[figure][2][]2}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{3}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \numprint  {2048} densely connected neurons with different neuron-dependent update and transfer functions (4 neuron types), in the presence of external inputs. The training dataset contains $10^5$ time points. (\textbf  {a}) External inputs are represented by a time-dependent scalar field $\Omega _i(t)$ that scales the connectivity matrix $\ensuremath  {\boldsymbol  {W}}_{ij}$ (\cref  {eqn:simulation2}). 1024 neurons (left), spatially ordered, are modulated by this field. The other 1024 neurons (right) are not affected ($\Omega _i=1$). (\textbf  {b}) Activity time values. (\textbf  {c}) Sample of 10 time series used for training. (\textbf  {d}) Comparison of learned and true connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$ (given $g_i=10$ in \cref  {eqn:simulation}). (\textbf  {e}) Comparison of learned and true $\Omega _i(t)$ values. (\textbf  {f}) True field $\Omega _i(t)$ plotted at different time-points. (\textbf  {g})~Learned field $\Omega _i^*(t)$ plotted at different time-points.}}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig3}{{3}{4}{\np {2048} densely connected neurons with different neuron-dependent update and transfer functions (4 neuron types), in the presence of external inputs. The training dataset contains $10^5$ time points. (\textbf {a}) External inputs are represented by a time-dependent scalar field $\Omega _i(t)$ that scales the connectivity matrix $\Mat {W}_{ij}$ (\cref {eqn:simulation2}). 1024 neurons (left), spatially ordered, are modulated by this field. The other 1024 neurons (right) are not affected ($\Omega _i=1$). (\textbf {b}) Activity time values. (\textbf {c}) Sample of 10 time series used for training. (\textbf {d}) Comparison of learned and true connectivity $\Mat {W}_{ij}$ (given $g_i=10$ in \cref {eqn:simulation}). (\textbf {e}) Comparison of learned and true $\Omega _i(t)$ values. (\textbf {f}) True field $\Omega _i(t)$ plotted at different time-points. (\textbf {g})~Learned field $\Omega _i^*(t)$ plotted at different time-points}{figure.caption.4}{}}
\newlabel{fig3@cref}{{[figure][3][]3}{[1][3][]4}{}{}{}}
\abx@aux@cite{0}{lueckmann_zapbench_2025}
\abx@aux@segm{0}{0}{lueckmann_zapbench_2025}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Supplementary notes}{6}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Neuron type clustering used during training}{6}{subsection.A.1}\protected@file@percent }
\newlabel{appendix:cluster-training}{{A.1}{6}{Neuron type clustering used during training}{subsection.A.1}{}}
\newlabel{appendix:cluster-training@cref}{{[subsection][1][1]A.1}{[1][6][]6}{}{}{}}
\abx@aux@cite{0}{sitzmann_implicit_2020}
\abx@aux@segm{0}{0}{sitzmann_implicit_2020}
\newlabel{eqn:simulation3}{{1}{7}{Neuron type clustering used during training}{equation.1}{}}
\newlabel{eqn:simulation3@cref}{{[appendixequation][1][]1}{[1][7][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Simulation parameters. Connectivity indicates percentage of non-zero $W_{ij}$. Noise $\sigma ^2$ is variance of $\eta _i(t)$ in \cref  {eqn:simulation3}. $\Omega $ indicates presence of external inputs $\Omega _i(t)$. Parameters: $g_i$ (coupling strength), $s_i$ (self-coupling), $\tau _i$ (time constant), $\gamma _j$ (scale in $\tanh (x_j/\gamma _i)$), $\theta _j$ (linear term in $\tanh (x_j/\gamma _i)-\theta _j x_j$). }}{7}{table.caption.7}\protected@file@percent }
\newlabel{table_sim_parameters}{{1}{7}{Simulation parameters. Connectivity indicates percentage of non-zero $W_{ij}$. Noise $\sigma ^2$ is variance of $\eta _i(t)$ in \cref {eqn:simulation3}. $\Omega $ indicates presence of external inputs $\Omega _i(t)$. Parameters: $g_i$ (coupling strength), $s_i$ (self-coupling), $\tau _i$ (time constant), $\gamma _j$ (scale in $\tanh (x_j/\gamma _i)$), $\theta _j$ (linear term in $\tanh (x_j/\gamma _i)-\theta _j x_j$)}{table.caption.7}{}}
\newlabel{table_sim_parameters@cref}{{[appendixtable][1][]1}{[1][7][]7}{}{}{}}
\newlabel{eqn:GNN2}{{2}{7}{Neuron type clustering used during training}{equation.2}{}}
\newlabel{eqn:GNN2@cref}{{[appendixequation][2][]2}{[1][7][]7}{}{}{}}
\newlabel{eqn:loss_prediction2}{{3}{7}{Neuron type clustering used during training}{equation.3}{}}
\newlabel{eqn:loss_prediction2@cref}{{[appendixequation][3][]3}{[1][7][]7}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Training parameters for loss function (\cref  {eqn:loss_prediction2}).}}{7}{table.caption.8}\protected@file@percent }
\newlabel{table_train_parameters}{{2}{7}{Training parameters for loss function (\cref {eqn:loss_prediction2})}{table.caption.8}{}}
\newlabel{table_train_parameters@cref}{{[appendixtable][2][]2}{[1][7][]7}{}{}{}}
\abx@aux@cite{0}{cranmer_interpretable_2023}
\abx@aux@segm{0}{0}{cranmer_interpretable_2023}
\abx@aux@cite{0}{cranmer_interpretable_2023}
\abx@aux@segm{0}{0}{cranmer_interpretable_2023}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Comparison of true and learned functions plotted in \cref  {fig2}. Symbolic regression (PySR package \citep {cranmer_interpretable_2023}) is applied to the learned functions to retrieve their expressions.}}{8}{table.caption.9}\protected@file@percent }
\newlabel{table1}{{3}{8}{Comparison of true and learned functions plotted in \cref {fig2}. Symbolic regression (PySR package \citep {cranmer_interpretable_2023}) is applied to the learned functions to retrieve their expressions}{table.caption.9}{}}
\newlabel{table1@cref}{{[appendixtable][3][]3}{[1][8][]8}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Comparison of true and learned functions plotted in \cref  {supp8}.}}{8}{table.caption.10}\protected@file@percent }
\newlabel{table2}{{4}{8}{Comparison of true and learned functions plotted in \cref {supp8}}{table.caption.10}{}}
\newlabel{table2@cref}{{[appendixtable][4][]4}{[1][8][]8}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Comparison of true and learned functions plotted in \cref  {supp9}.}}{8}{table.caption.11}\protected@file@percent }
\newlabel{table3}{{5}{8}{Comparison of true and learned functions plotted in \cref {supp9}}{table.caption.11}{}}
\newlabel{table3@cref}{{[appendixtable][5][]5}{[1][8][]8}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \numprint  {1000} densely connected neurons with 4 neuron-dependent update functions. Results plotted over 20 epochs. (\textbf  {a}) Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ of all neurons. (\textbf  {b})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {c}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. (\textbf  {d})~Learned connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. (\textbf  {e}) Comparison of learned and true connectivity. Colors indicate true neuron types.}}{9}{figure.caption.12}\protected@file@percent }
\newlabel{supp1}{{1}{9}{\np {1000} densely connected neurons with 4 neuron-dependent update functions. Results plotted over 20 epochs. (\textbf {a}) Learned latent vectors $\Vec {a}_i$ of all neurons. (\textbf {b})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {c}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. (\textbf {d})~Learned connectivity $\Mat {W}_{ij}$. (\textbf {e}) Comparison of learned and true connectivity. Colors indicate true neuron types}{figure.caption.12}{}}
\newlabel{supp1@cref}{{[appendixfigure][1][]1}{[1][8][]9}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Rollout inference performed with the GNN model trained with a simulation of \numprint  {1000} densely connected neurons (\cref  {fig2}). Results plotted at time step 400 and 800, respectively. (\textbf  {a}) and (\textbf  {c}) 25 learned activity traces plotted as a function of time-points. True activity traces are overlaid in light gray. (\textbf  {b}) and (\textbf  {d}) Comparison between true and learned activity values of 1000 neurons. }}{10}{figure.caption.13}\protected@file@percent }
\newlabel{supp12}{{2}{10}{Rollout inference performed with the GNN model trained with a simulation of \np {1000} densely connected neurons (\cref {fig2}). Results plotted at time step 400 and 800, respectively. (\textbf {a}) and (\textbf {c}) 25 learned activity traces plotted as a function of time-points. True activity traces are overlaid in light gray. (\textbf {b}) and (\textbf {d}) Comparison between true and learned activity values of 1000 neurons}{figure.caption.13}{}}
\newlabel{supp12@cref}{{[appendixfigure][2][]2}{[1][8][]10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \numprint  {1000} densely connected neurons with 4 neuron dependent update functions (first two terms in \cref  {eqn:simulation}). Results are obtained with a GNN trained with the hypothesis that all neurons are identical. To do so, we fixed the learnable latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ to a unique vector. (\textbf  {a}) Activity time series used for GNN training. This dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c}) True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. The inset shows $20\times 20$ weights. (\textbf  {d})~Learned connectivity. (\textbf  {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref  {eqn:simulation}). (\textbf  {f}) Fixed latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ of all neurons. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}}{10}{figure.caption.14}\protected@file@percent }
\newlabel{supp13}{{3}{10}{\np {1000} densely connected neurons with 4 neuron dependent update functions (first two terms in \cref {eqn:simulation}). Results are obtained with a GNN trained with the hypothesis that all neurons are identical. To do so, we fixed the learnable latent vectors $\Vec {a}_i$ to a unique vector. (\textbf {a}) Activity time series used for GNN training. This dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c}) True connectivity $\Mat {W}_{ij}$. The inset shows $20\times 20$ weights. (\textbf {d})~Learned connectivity. (\textbf {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref {eqn:simulation}). (\textbf {f}) Fixed latent vectors $\Vec {a}_i$ of all neurons. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.14}{}}
\newlabel{supp13@cref}{{[appendixfigure][3][]3}{[1][8][]10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Rollout inference performed with the GNN model trained with the hypothesis that all neurons are identical (\cref  {supp13}). Results plotted at time step 200 and 800, respectively. (\textbf  {a}) and (\textbf  {c}) 25 learned activity traces plotted as a function of time-points. True activity traces are overlaid in light gray. (\textbf  {b}) and (\textbf  {d}) Comparison between true and learned activity values of 1000 neurons. }}{10}{figure.caption.15}\protected@file@percent }
\newlabel{supp14}{{4}{10}{Rollout inference performed with the GNN model trained with the hypothesis that all neurons are identical (\cref {supp13}). Results plotted at time step 200 and 800, respectively. (\textbf {a}) and (\textbf {c}) 25 learned activity traces plotted as a function of time-points. True activity traces are overlaid in light gray. (\textbf {b}) and (\textbf {d}) Comparison between true and learned activity values of 1000 neurons}{figure.caption.15}{}}
\newlabel{supp14@cref}{{[appendixfigure][4][]4}{[1][8][]10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Generalization test with modified network structure: performance evaluation after changing connectivity matrix and neuron type proportions. The GNN model was trained with \numprint  {1000} densely connected neurons. (\textbf  {a}) Original relative proportions of neuron types (25\% each). (\textbf  {b}) Modified relative proportions of neuron types (10\%, 20\%, 30\%, 40\%). (\textbf  {c}) Original connectivity matrix ($10^6$ weights, fully connected). (\textbf  {d}) Modified sparse connectivity matrix (243,831 weights, $\sim $25\% sparsity). (\textbf  {e,f}) Rollout inference over 400 time-steps shows perfect performance ($R^2=1.0$, slope$=1.0$). (\textbf  {g,h}) Extended rollout over 800 time-steps maintains high accuracy ($R^2=0.996$, slope$=1.0$).}}{10}{figure.caption.16}\protected@file@percent }
\newlabel{supp18}{{5}{10}{Generalization test with modified network structure: performance evaluation after changing connectivity matrix and neuron type proportions. The GNN model was trained with \np {1000} densely connected neurons. (\textbf {a}) Original relative proportions of neuron types (25\% each). (\textbf {b}) Modified relative proportions of neuron types (10\%, 20\%, 30\%, 40\%). (\textbf {c}) Original connectivity matrix ($10^6$ weights, fully connected). (\textbf {d}) Modified sparse connectivity matrix (243,831 weights, $\sim $25\% sparsity). (\textbf {e,f}) Rollout inference over 400 time-steps shows perfect performance ($R^2=1.0$, slope$=1.0$). (\textbf {g,h}) Extended rollout over 800 time-steps maintains high accuracy ($R^2=0.996$, slope$=1.0$)}{figure.caption.16}{}}
\newlabel{supp18@cref}{{[appendixfigure][5][]5}{[1][8][]10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Generalization test with modified network structure. (\textbf  {a}) Original relative proportions of neuron types (25\% each). (\textbf  {b}) Modified relative proportions with only two types present (60\%, 40\%, types 2 and 3 eliminated). (\textbf  {c}) Original connectivity matrix ($10^6$ weights, fully connected). (\textbf  {d}) Modified sparse connectivity matrix (487,401 weights, 50\% sparsity). (\textbf  {e,f}) Rollout inference over 400 time-steps achieves excellent accuracy ($R^2=1.0$, slope$=1.0$). (\textbf  {g,h}) Extended rollout over 800 time-steps maintains high accuracy ($R^2=0.975$, slope$=0.99$).}}{11}{figure.caption.17}\protected@file@percent }
\newlabel{supp19}{{6}{11}{Generalization test with modified network structure. (\textbf {a}) Original relative proportions of neuron types (25\% each). (\textbf {b}) Modified relative proportions with only two types present (60\%, 40\%, types 2 and 3 eliminated). (\textbf {c}) Original connectivity matrix ($10^6$ weights, fully connected). (\textbf {d}) Modified sparse connectivity matrix (487,401 weights, 50\% sparsity). (\textbf {e,f}) Rollout inference over 400 time-steps achieves excellent accuracy ($R^2=1.0$, slope$=1.0$). (\textbf {g,h}) Extended rollout over 800 time-steps maintains high accuracy ($R^2=0.975$, slope$=0.99$)}{figure.caption.17}{}}
\newlabel{supp19@cref}{{[appendixfigure][6][]6}{[1][8][]11}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \numprint  {1000} densely connected neurons with 4 neuron-dependent update functions. The plot displays $R^2$ for the comparison between true and learned connectivity matrices $\ensuremath  {\boldsymbol  {W}}_{ij}$ as a function of training epochs for different training dataset sizes (colors). Comparison is made at equal numbers of gradient descent iterations.}}{11}{figure.caption.18}\protected@file@percent }
\newlabel{supp2}{{7}{11}{\np {1000} densely connected neurons with 4 neuron-dependent update functions. The plot displays $R^2$ for the comparison between true and learned connectivity matrices $\Mat {W}_{ij}$ as a function of training epochs for different training dataset sizes (colors). Comparison is made at equal numbers of gradient descent iterations}{figure.caption.18}{}}
\newlabel{supp2@cref}{{[appendixfigure][7][]7}{[1][8][]11}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces \numprint  {1000} sparsely (5\%) connected neurons with 4 neuron-dependent update functions. Results are obtained after 20 epochs. (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Time series of a sample of 10 representative neurons taken from (\textbf  {a}). (\textbf  {c}) True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. The inset shows $20\times 20$ weights. (\textbf  {d})~Learned connectivity. (\textbf  {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref  {eqn:simulation}). (\textbf  {f}) Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ of all neurons. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}}{11}{figure.caption.19}\protected@file@percent }
\newlabel{supp4}{{8}{11}{\np {1000} sparsely (5\%) connected neurons with 4 neuron-dependent update functions. Results are obtained after 20 epochs. (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Time series of a sample of 10 representative neurons taken from (\textbf {a}). (\textbf {c}) True connectivity $\Mat {W}_{ij}$. The inset shows $20\times 20$ weights. (\textbf {d})~Learned connectivity. (\textbf {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref {eqn:simulation}). (\textbf {f}) Learned latent vectors $\Vec {a}_i$ of all neurons. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.19}{}}
\newlabel{supp4@cref}{{[appendixfigure][8][]8}{[1][8][]11}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces \numprint  {1000} densely connected neurons with 4 neuron-dependent update functions. The plot displays $R^2$ for the comparison between true and learned connectivity matrices $\ensuremath  {\boldsymbol  {W}}_{ij}$ as a function of training epochs for different connectivity filling factors (colors). All comparisons are made at equal numbers of gradient descent iterations.}}{11}{figure.caption.20}\protected@file@percent }
\newlabel{supp5}{{9}{11}{\np {1000} densely connected neurons with 4 neuron-dependent update functions. The plot displays $R^2$ for the comparison between true and learned connectivity matrices $\Mat {W}_{ij}$ as a function of training epochs for different connectivity filling factors (colors). All comparisons are made at equal numbers of gradient descent iterations}{figure.caption.20}{}}
\newlabel{supp5@cref}{{[appendixfigure][9][]9}{[1][8][]11}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces \numprint  {1000} densely connected neurons with 4 neuron-dependent update functions in the presence of Gaussian noise (\cref  {eqn:simulation2}). The signal-to-noise ratio is about 10 dB as measured by comparing filtered and raw signals. For comparison, corresponding signals without noise are shown in \cref  {fig2}. (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c}) True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. The inset shows $20\times 20$ weights. (\textbf  {d})~Learned connectivity. (\textbf  {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref  {eqn:simulation}). (\textbf  {f}) Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ of all neurons. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}}{12}{figure.caption.21}\protected@file@percent }
\newlabel{supp6}{{10}{12}{\np {1000} densely connected neurons with 4 neuron-dependent update functions in the presence of Gaussian noise (\cref {eqn:simulation2}). The signal-to-noise ratio is about 10 dB as measured by comparing filtered and raw signals. For comparison, corresponding signals without noise are shown in \cref {fig2}. (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c}) True connectivity $\Mat {W}_{ij}$. The inset shows $20\times 20$ weights. (\textbf {d})~Learned connectivity. (\textbf {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref {eqn:simulation}). (\textbf {f}) Learned latent vectors $\Vec {a}_i$ of all neurons. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.21}{}}
\newlabel{supp6@cref}{{[appendixfigure][10][]10}{[1][8][]12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces \numprint  {8000} densely connected neurons with 4 neuron-dependent update functions. Results obtained after 14 epochs. (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c}) True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. The inset shows $20\times 20$ weights. (\textbf  {d})~Learned connectivity. (\textbf  {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref  {eqn:simulation}). (\textbf  {f}) Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$ of all neurons. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray.}}{12}{figure.caption.22}\protected@file@percent }
\newlabel{supp3}{{11}{12}{\np {8000} densely connected neurons with 4 neuron-dependent update functions. Results obtained after 14 epochs. (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c}) True connectivity $\Mat {W}_{ij}$. The inset shows $20\times 20$ weights. (\textbf {d})~Learned connectivity. (\textbf {e}) Comparison of learned and true connectivity (given $g_i=10$ in \cref {eqn:simulation}). (\textbf {f}) Learned latent vectors $\Vec {a}_i$ of all neurons. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer function $\psi ^*(x)$, normalized to a maximum value of 1. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.22}{}}
\newlabel{supp3@cref}{{[appendixfigure][11][]11}{[1][8][]12}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces \numprint  {1000} densely connected neurons with 32 neuron-dependent update functions. Results are obtained after 20 epochs. (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c}) True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. (\textbf  {d}) Learned connectivity. (\textbf  {e}) Comparison between learned and true connectivity. (\textbf  {f}) Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$. (\textbf  {g}) Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer functions $\psi ^*(x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}}{13}{figure.caption.23}\protected@file@percent }
\newlabel{supp7}{{12}{13}{\np {1000} densely connected neurons with 32 neuron-dependent update functions. Results are obtained after 20 epochs. (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c}) True connectivity $\Mat {W}_{ij}$. (\textbf {d}) Learned connectivity. (\textbf {e}) Comparison between learned and true connectivity. (\textbf {f}) Learned latent vectors $\Vec {a}_i$. (\textbf {g}) Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer functions $\psi ^*(x)$. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.23}{}}
\newlabel{supp7@cref}{{[appendixfigure][12][]12}{[1][8][]13}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \numprint  {1000} densely connected neurons with neuron-dependent update and transfer functions (4 neuron types). (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c})~True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. (\textbf  {d})~Learned connectivity. (\textbf  {e})~Comparison between learned and true connectivity. (\textbf  {f})~Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer functions $\psi ^*(\ensuremath  {\boldsymbol  {a}},x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}}{13}{figure.caption.24}\protected@file@percent }
\newlabel{supp8}{{13}{13}{\np {1000} densely connected neurons with neuron-dependent update and transfer functions (4 neuron types). (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c})~True connectivity $\Mat {W}_{ij}$. (\textbf {d})~Learned connectivity. (\textbf {e})~Comparison between learned and true connectivity. (\textbf {f})~Learned latent vectors $\Vec {a}_i$. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer functions $\psi ^*(\Vec {a},x)$. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.24}{}}
\newlabel{supp8@cref}{{[appendixfigure][13][]13}{[1][8][]13}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces \numprint  {1000} densely connected neurons with neuron-dependent update and transfer functions (4 neuron types). (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c})~True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. (\textbf  {d})~Learned connectivity. (\textbf  {e})~Comparison between learned and true connectivity. (\textbf  {f})~Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. (\textbf  {h}) Learned transfer functions $\psi ^*(\ensuremath  {\boldsymbol  {a_i}},\ensuremath  {\boldsymbol  {a_j}}, x)$. Colors indicate true neuron types. True functions are overlaid in light gray.}}{14}{figure.caption.25}\protected@file@percent }
\newlabel{supp9}{{14}{14}{\np {1000} densely connected neurons with neuron-dependent update and transfer functions (4 neuron types). (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c})~True connectivity $\Mat {W}_{ij}$. (\textbf {d})~Learned connectivity. (\textbf {e})~Comparison between learned and true connectivity. (\textbf {f})~Learned latent vectors $\Vec {a}_i$. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. (\textbf {h}) Learned transfer functions $\psi ^*(\Vec {a_i},\Vec {a_j}, x)$. Colors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.25}{}}
\newlabel{supp9@cref}{{[appendixfigure][14][]14}{[1][8][]14}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces \numprint  {2048} densely connected neurons with neuron-dependent update and transfer functions (4 neuron types) in the presence of external stimuli. Results are obtained after 16 epochs. (\textbf  {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf  {b}) Sample of 10 time series taken from (\textbf  {a}). (\textbf  {c})~True connectivity $\ensuremath  {\boldsymbol  {W}}_{ij}$. (\textbf  {d})~Learned connectivity. (\textbf  {e})~Comparison between learned and true connectivity. (\textbf  {f})~Learned latent vectors $\ensuremath  {\boldsymbol  {a}}_i$. (\textbf  {g})~Learned update functions $\phi ^*(\ensuremath  {\boldsymbol  {a}}, x)$. particleColors indicate true neuron types. True functions are overlaid in light gray.}}{14}{figure.caption.26}\protected@file@percent }
\newlabel{supp10}{{15}{14}{\np {2048} densely connected neurons with neuron-dependent update and transfer functions (4 neuron types) in the presence of external stimuli. Results are obtained after 16 epochs. (\textbf {a}) Activity time series used for GNN training. The training dataset contains $10^5$ time-points. (\textbf {b}) Sample of 10 time series taken from (\textbf {a}). (\textbf {c})~True connectivity $\Mat {W}_{ij}$. (\textbf {d})~Learned connectivity. (\textbf {e})~Comparison between learned and true connectivity. (\textbf {f})~Learned latent vectors $\Vec {a}_i$. (\textbf {g})~Learned update functions $\phi ^*(\Vec {a}, x)$. particleColors indicate true neuron types. True functions are overlaid in light gray}{figure.caption.26}{}}
\newlabel{supp10@cref}{{[appendixfigure][15][]15}{[1][8][]14}{}{}{}}
\abx@aux@read@bbl@mdfivesum{nohash}
\abx@aux@read@bblrerun
\gdef \@abspage@last{14}
