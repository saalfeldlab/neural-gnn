[
  {
    "objectID": "Notebook_10.html",
    "href": "Notebook_10.html",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "",
    "text": "This script reproduces paper’s Figure 3. We tested whether we could recover both network structure and dynamics, as well as unknown external inputs\nSimulation parameters:\nThe simulation follows:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\tanh(x_i) + g_i \\Omega_i(t) \\sum_j W_{ij} \\tanh\\left(\\frac{x_j}{\\gamma_j}\\right) + \\eta_i(t)\\]\nThe GNN learns:\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\Omega_i^*(t) \\sum_j W_{ij} \\psi^*(\\mathbf{a}_j, x_j)\\]\nThe external input \\(\\Omega_i(t)\\) is scalar field that modulates the connectivity for the first 1024 neurons. The neuron index \\(i\\) corresponds to a known spatial coordinate \\(x_i\\). The remaining 1024 neurons have \\(\\Omega_i = 1\\)."
  },
  {
    "objectID": "Notebook_10.html#configuration-and-setup",
    "href": "Notebook_10.html#configuration-and-setup",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Figure 3: 2048 neurons, 4 types, with external inputs Omega(t)\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_3'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_10.html#step-1-generate-data",
    "href": "Notebook_10.html#step-1-generate-data",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N4 model. This creates the training dataset with 2048 neurons and external inputs.\nOutputs:\n\nFigure 3a: External inputs Omega_i(t) - time-dependent scalar field\nFigure 3b: Activity time series\nFigure 3c: Sample of 100 time series\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity with external inputs (Fig 3a-c)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"external inputs: {config.simulation.n_input_neurons} neurons modulated by Omega(t)\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nFig 3a-b: (Top) External input field \\(\\Omega_i(t)\\) shown on a 32×32 grid (left, first 1024 neurons) and sunflower arrangement (right, remaining 1024 neurons). (Bottom) Neural activity \\(x_i\\) at time \\(t=0\\).\n\n\n\n\n\n\n\n\n\nFig 3c: Sample activity time series for 100 neurons over 10,000 time steps. Y-axis shows neuron index."
  },
  {
    "objectID": "Notebook_10.html#step-2-train-gnn",
    "href": "Notebook_10.html#step-2-train-gnn",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\), latent embeddings \\(\\mathbf{a}_i\\), functions \\(\\phi^*/\\psi^*\\), and the external input field \\(\\Omega^*(x, y, t)\\) using a coordinate-based MLP (SIREN).\nLearning targets:\n\nConnectivity matrix \\(W\\)\nLatent vectors \\(\\mathbf{a}_i\\)\nUpdate function \\(\\phi^*(\\mathbf{a}_i, x)\\)\nTransfer function \\(\\psi^*(x)\\)\nExternal input field \\(\\Omega^*(x, y, t)\\) via SIREN network\n\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi, and Omega*\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi*, psi*\")\n    print(f\"learning: external input field Omega*(x, y, t) via SIREN network\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_10.html#step-3-generate-publication-figures",
    "href": "Notebook_10.html#step-3-generate-publication-figures",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Step 3: Generate Publication Figures",
    "text": "Step 3: Generate Publication Figures\nGenerate publication-quality figures matching Figure 3 from the paper.\nFigure panels:\n\nFig 3d: Comparison of learned vs true connectivity W_ij\nFig 3e: Comparison of learned vs true Omega_i(t) values\nFig 3f: True field Omega_i(t) at different time-points\nFig 3g: Learned field Omega*(t) at different time-points\n\n\n\nCode\n# STEP 3: PLOT\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: PLOT - Generating Figure 3 panels (d-g)\")\nprint(\"-\" * 80)\nprint(f\"Fig 3d: W learned vs true (R^2, slope)\")\nprint(f\"Fig 3e: Omega learned vs true\")\nprint(f\"Fig 3f: True field Omega_i(t) at different times\")\nprint(f\"Fig 3g: Learned field Omega*(t) at different times\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)"
  },
  {
    "objectID": "Notebook_10.html#output-files",
    "href": "Notebook_10.html#output-files",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Output Files",
    "text": "Output Files\nRename output files to match Figure 3 panels.\n\n\nCode\n# Rename output files to match Figure 3 panels\nprint()\nprint(\"-\" * 80)\nprint(\"renaming output files to Figure 3 panels\")\nprint(\"-\" * 80)\n\nresults_dir = f'{log_dir}/results'\nos.makedirs(results_dir, exist_ok=True)\n\n# File mapping for simple copies\nfile_mapping = {\n    f'{graphs_dir}/activity_sample.png': f'{results_dir}/Fig3d_activity_sample.png',\n    f'{results_dir}/weights_comparison_corrected.png': f'{results_dir}/Fig3e_weights_comparison.png',\n}\n\nfor src, dst in file_mapping.items():\n    if os.path.exists(src):\n        shutil.copy2(src, dst)\n        print(f\"{os.path.basename(dst)}\")\n\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Copy Fig 3a-b from generated frame (plot_synaptic_frame_visual output)\nfig_file = f'{graphs_dir}/Fig/Fig_0_000000.png'\nif os.path.exists(fig_file):\n    shutil.copy2(fig_file, f'{results_dir}/Fig3ab_external_input_activity.png')\n    print(f\"Fig3ab_external_input_activity.png\")\n\n# Copy Fig 3c: Activity time series\nif os.path.exists(f'{graphs_dir}/activity.png'):\n    shutil.copy2(f'{graphs_dir}/activity.png', f'{results_dir}/Fig3c_activity_time_series.png')\n    print(f\"Fig3c_activity_time_series.png\")\n\n# Generate Fig 3f: True field Omega_i(t) montage from field images\nprint(\"generating Fig3f_omega_field_true.png (5-frame montage)...\")\nfield_dir = f'{results_dir}/field'\nframe_indices = [0, 10000, 20000, 30000, 40000]\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, frame in enumerate(frame_indices):\n    ax = axes[idx]\n    # Find true field image for this frame\n    true_field_files = sorted(glob.glob(f'{field_dir}/true_field*_{frame}.png'))\n    if true_field_files:\n        img = mpimg.imread(true_field_files[-1])\n        ax.imshow(img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f't={frame}', fontsize=12)\n    ax.axis('off')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/Fig3f_omega_field_true.png', dpi=150)\nplt.close()\nprint(f\"Fig3f_omega_field_true.png\")\n\n# Generate Fig 3g: Learned field Omega*(t) montage from field images\nprint(\"generating Fig3g_omega_field_learned.png (5-frame montage)...\")\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, frame in enumerate(frame_indices):\n    ax = axes[idx]\n    # Find learned field image for this frame\n    learned_field_files = sorted(glob.glob(f'{field_dir}/reconstructed_field_LR*_{frame}.png'))\n    if learned_field_files:\n        img = mpimg.imread(learned_field_files[-1])\n        ax.imshow(img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f't={frame}', fontsize=12)\n    ax.axis('off')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/Fig3g_omega_field_learned.png', dpi=150)\nplt.close()\nprint(f\"Fig3g_omega_field_learned.png\")\n\nprint()\nprint(\"=\" * 80)\nprint(\"Figure 3 complete!\")\nprint(f\"results saved to: {log_dir}/results/\")\nprint(\"=\" * 80)"
  },
  {
    "objectID": "Notebook_10.html#figure-3-panels",
    "href": "Notebook_10.html#figure-3-panels",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Figure 3 Panels",
    "text": "Figure 3 Panels\n\n\n\n\n\nFig 3d: Comparison of learned and true connectivity.\n\n\n\n\n\n\n\n\n\nFig 3e: Comparison of learned and true \\(\\Omega_i\\) values.\n\n\n\n\n\nFig 3f-g: True and Learned External Input Fields\nShowing \\(\\Omega_i(t)\\) at frames 0, 10000, 20000, 30000, 40000.\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 0.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 0.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 10000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 10000.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 20000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 20000.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 30000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 30000.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 40000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 40000."
  },
  {
    "objectID": "Notebook_10.html#supplementary-figure-15-learned-functions",
    "href": "Notebook_10.html#supplementary-figure-15-learned-functions",
    "title": "Figure 3: External Inputs - 2048 neurons with external inputs",
    "section": "Supplementary Figure 15: Learned Functions",
    "text": "Supplementary Figure 15: Learned Functions\nLearned latent embeddings and functions from Figure 3 training.\n\n\n\n\n\nSupp. Fig 15f: Learned latent vectors \\(a_i\\).\n\n\n\n\n\n\n\n\n\nSupp. Fig 15g: Learned update functions \\(\\phi^*(a, x)\\). Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 15h: : Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True functions are overlaid in light gray."
  },
  {
    "objectID": "Notebook_08.html",
    "href": "Notebook_08.html",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 13. Test with neuron-specific transfer functions of the form \\(\\psi(x_j/\\gamma_i)\\).\nSimulation parameters:\nThe simulation follows an extended version of Equation 2:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh\\left(\\frac{x_j}{\\gamma_i}\\right)\\]\nThe GNN jointly optimizes the shared MLP \\(\\psi^*\\) and latent vectors \\(\\mathbf{a}_i\\) to accurately identify the neuron-specific transfer functions."
  },
  {
    "objectID": "Notebook_08.html#configuration-and-setup",
    "href": "Notebook_08.html#configuration-and-setup",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 13: 1000 neurons, 4 types, heterogeneous transfer functions\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_13'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_08.html#step-1-generate-data",
    "href": "Notebook_08.html#step-1-generate-data",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N4 model with neuron-specific transfer functions. Each neuron type has a different parameter \\(\\gamma_i\\) in the transfer function \\(\\psi(x_j/\\gamma_i)\\).\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (heterogeneous transfer functions)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"transfer function gamma_i = [1, 2, 4, 8]\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nSupp. Fig 13b: Sample time series taken from the activity data (heterogeneous transfer functions).\n\n\n\n\n\n\n\n\n\nSupp. Fig 13c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_08.html#step-2-train-gnn",
    "href": "Notebook_08.html#step-2-train-gnn",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\), latent embeddings \\(\\mathbf{a}_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN must learn neuron-specific transfer functions \\(\\psi(x_j/\\gamma_i)\\).\nThe GNN optimizes the update rule:\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(\\mathbf{a}_j, x_j)\\]\nwhere the transfer function \\(\\psi^*\\) now depends on the latent vector \\(\\mathbf{a}_j\\).\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn heterogeneous transfer functions\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, neuron-specific psi*(a_j, x_j)\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_08.html#step-3-gnn-evaluation",
    "href": "Notebook_08.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 13 from the paper.\nFigure panels:\n\nSupp. Fig 13d: Learned connectivity matrix\nSupp. Fig 13e: Comparison of learned vs true connectivity (expected: \\(R^2\\)=0.99, slope=0.99)\nSupp. Fig 13f: Learned latent vectors \\(\\mathbf{a}_i\\)\nSupp. Fig 13g: Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nSupp. Fig 13h: Learned transfer functions \\(\\psi^*(\\mathbf{a}_j, x)\\)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 13 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (4 clusters)\")\nprint(f\"update functions phi*(a_i, x)\")\nprint(f\"transfer functions psi*(a_j, x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nSupplementary Figure 13: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 13d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nSupp. Fig 13e: Comparison of learned and true connectivity (given \\(g_i\\)=10). Expected: \\(R^2\\)=0.99, slope=0.99.\n\n\n\n\n\n\n\n\n\nSupp. Fig 13f: Learned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nSupp. Fig 13g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 13h: Learned transfer functions \\(\\psi^*(a_j, x)\\) (\\(\\gamma\\)=1,2,4,8), normalized to a maximum value of 1. True functions are overlaid in light gray."
  },
  {
    "objectID": "Notebook_06.html",
    "href": "Notebook_06.html",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 11. Large scale test with 8,000 neurons and 64 million connectivity weights.\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j) + \\eta_i(t)\\]"
  },
  {
    "objectID": "Notebook_06.html#configuration-and-setup",
    "href": "Notebook_06.html#configuration-and-setup",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 11: 8000 neurons, 4 types, dense connectivity (64M weights)\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_11'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_06.html#step-1-generate-data",
    "href": "Notebook_06.html#step-1-generate-data",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model with 8000 neurons. This creates a large-scale training dataset with 64 million connectivity weights.\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\) (8000×8000 = 64M weights)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (8000 neurons)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"connectivity matrix: {config.simulation.n_neurons}x{config.simulation.n_neurons} = 64M weights\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nSupp. Fig 11b: Sample time series taken from the activity data (8000 neurons).\n\n\n\n\n\n\n\n\n\nSupp. Fig 11c: True connectivity \\(W_{ij}\\) (8000×8000 = 64 million weights). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_06.html#step-2-train-gnn",
    "href": "Notebook_06.html#step-2-train-gnn",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn the 64 million connectivity weights, latent embeddings \\(\\mathbf{a}_i\\), and functions \\(\\phi^*, \\psi^*\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn 64M connectivity weights\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: 64M connectivity weights, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_06.html#step-3-gnn-evaluation",
    "href": "Notebook_06.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 11 from the paper.\nFigure panels:\n\nSupp. Fig 11d: Learned connectivity matrix (64M weights)\nSupp. Fig 11e: Comparison of learned vs true connectivity\nSupp. Fig 11f: Learned latent vectors \\(\\mathbf{a}_i\\)\nSupp. Fig 11g: Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nSupp. Fig 11h: Learned transfer function \\(\\psi^*(x)\\)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 11 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix (64M weights)\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (4 clusters)\")\nprint(f\"update functions phi*(a_i, x)\")\nprint(f\"transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nSupplementary Figure 11: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 11d: Learned connectivity (8000×8000 = 64 million weights).\n\n\n\n\n\n\n\n\n\nSupp. Fig 11e: Comparison of learned and true connectivity (given \\(g_i\\)=10). Expected: \\(R^2\\)=1.00, slope=1.00.\n\n\n\n\n\n\n\n\n\nSupp. Fig 11f: Learned latent vectors \\(a_i\\) of all 8000 neurons.\n\n\n\n\n\n\n\n\n\nSupp. Fig 11g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 8000 overlaid curves. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 11h: Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_04.html",
    "href": "Notebook_04.html",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figures 8 and 9. Performance of GNN for connectivity matrices with varying sparsity levels. This notebook displays connectivity matrix comparison, \\(\\phi^*\\) plots, \\(\\psi^*\\) plots, and learned embedding for each sparsity level.\nSimulation parameters (constant across all experiments):\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j)\\]\nVariable: Connectivity sparsity"
  },
  {
    "objectID": "Notebook_04.html#configuration",
    "href": "Notebook_04.html#configuration",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Configuration",
    "text": "Configuration\n\n\nCode\nimport glob\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 8: Effect of Connectivity Sparsity\")\nprint(\"=\" * 80)\n\n# All configs to process (config_name, sparsity)\nconfig_list = [\n    ('signal_fig_supp_8', '5%'),\n    ('signal_fig_supp_8_3', '10%'),\n    ('signal_fig_supp_8_2', '20%'),\n    ('signal_fig_supp_8_1', '50%'),\n    ('signal_fig_2', '100%'),\n]\n\ndevice = []\nbest_model = ''\nconfig_root = \"./config\""
  },
  {
    "objectID": "Notebook_04.html#steps-1-3-generate-train-and-plot-for-all-configs",
    "href": "Notebook_04.html#steps-1-3-generate-train-and-plot-for-all-configs",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Steps 1-3: Generate, Train, and Plot for all configs",
    "text": "Steps 1-3: Generate, Train, and Plot for all configs\nLoop over all sparsity levels: generate data, train GNN, and generate plots. Skips steps if data/models already exist.\n\n\nCode\nfor config_file_, sparsity in config_list:\n    print()\n    print(\"=\" * 80)\n    print(f\"Processing: {config_file_} ({sparsity} sparsity)\")\n    print(\"=\" * 80)\n\n    config_file, pre_folder = add_pre_folder(config_file_)\n\n    # Load config\n    config = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\n    config.config_file = config_file\n    config.dataset = config_file\n\n    if device == []:\n        device = set_device(config.training.device)\n\n    log_dir = f'./log/{config_file}'\n    graphs_dir = f'./graphs_data/{config_file}'\n\n    # STEP 1: GENERATE\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 1: GENERATE - Simulating neural activity\")\n    print(\"-\" * 80)\n\n    data_file = f'{graphs_dir}/x_list_0.npy'\n    if os.path.exists(data_file):\n        print(f\"data already exists at {graphs_dir}/\")\n        print(\"skipping simulation, regenerating figures...\")\n        data_generate(\n            config,\n            device=device,\n            visualize=False,\n            run_vizualized=0,\n            style=\"color\",\n            alpha=1,\n            erase=False,\n            bSave=True,\n            step=2,\n            regenerate_plots_only=True,\n        )\n    else:\n        print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_frames} frames\")\n        print(f\"output: {graphs_dir}/\")\n        data_generate(\n            config,\n            device=device,\n            visualize=False,\n            run_vizualized=0,\n            style=\"color\",\n            alpha=1,\n            erase=False,\n            bSave=True,\n            step=2,\n        )\n\n    # STEP 2: TRAIN\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 2: TRAIN - Training GNN\")\n    print(\"-\" * 80)\n\n    model_files = glob.glob(f'{log_dir}/models/*.pt')\n    if model_files:\n        print(f\"trained model already exists at {log_dir}/models/\")\n        print(\"skipping training (delete models folder to retrain)\")\n    else:\n        print(f\"training for {config.training.n_epochs} epochs\")\n        print(f\"sparsity: {sparsity}\")\n        data_train(\n            config=config,\n            erase=False,\n            best_model=best_model,\n            style='color',\n            device=device\n        )\n\n    # STEP 3: PLOT\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 3: PLOT - Generating figures\")\n    print(\"-\" * 80)\n\n    folder_name = f'{log_dir}/tmp_results/'\n    os.makedirs(folder_name, exist_ok=True)\n\n    data_plot(\n        config=config,\n        config_file=config_file,\n        epoch_list=['best'],\n        style='color',\n        extended='plots',\n        device=device,\n        apply_weight_correction=True,\n        plot_eigen_analysis=False\n    )\n\n    # STEP 4: TRAINING PROGRESSION (R² over iterations)\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 4: TRAINING PROGRESSION - Computing R² over iterations\")\n    print(\"-\" * 80)\n\n    r2_file = f'{log_dir}/results/all/r2_over_iterations.json'\n    if os.path.exists(r2_file):\n        print(f\"R² data already exists at {r2_file}\")\n        print(\"skipping (delete results/all/ folder to recompute)\")\n    else:\n        data_plot(\n            config=config,\n            config_file=config_file,\n            epoch_list=['all'],\n            style='color',\n            extended='plots',\n            device=device,\n            apply_weight_correction=True,\n            plot_eigen_analysis=False,\n        )"
  },
  {
    "objectID": "Notebook_04.html#activity-time-series",
    "href": "Notebook_04.html#activity-time-series",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Activity Time Series",
    "text": "Activity Time Series\nSample of 100 time series for each sparsity level.\n\n\n\n\n\nSupp. Fig 8b: Sample of 100 time series (5% sparsity)\n\n\n\n\n\n\n\n\n\nSample of 100 time series (10% sparsity)\n\n\n\n\n\n\n\n\n\nSample of 100 time series (20% sparsity)\n\n\n\n\n\n\n\n\n\nSample of 100 time series (50% sparsity)\n\n\n\n\n\n\n\n\n\nSample of 100 time series (100% connectivity)"
  },
  {
    "objectID": "Notebook_04.html#true-connectivity-matrix-w_ij",
    "href": "Notebook_04.html#true-connectivity-matrix-w_ij",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "True Connectivity Matrix \\(W_{ij}\\)",
    "text": "True Connectivity Matrix \\(W_{ij}\\)\nTrue connectivity matrix for each sparsity level.\n\n\n\n\n\nSupp. Fig 8c: True connectivity \\(W_{ij}\\) (5% sparsity)\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\) (10% sparsity)\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\) (20% sparsity)\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\) (50% sparsity)\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\) (100% connectivity)"
  },
  {
    "objectID": "Notebook_04.html#connectivity-matrix-comparison",
    "href": "Notebook_04.html#connectivity-matrix-comparison",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Connectivity Matrix Comparison",
    "text": "Connectivity Matrix Comparison\nLearned vs true connectivity matrix \\(W_{ij}\\) after training. The scatter plot shows \\(R^2\\) and slope metrics.\n\n\n\n\n\nSupp. Fig 8e: Connectivity comparison (5% sparsity)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (10% sparsity)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (20% sparsity)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (50% sparsity)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (100% connectivity)"
  },
  {
    "objectID": "Notebook_04.html#update-function-phimathbfa_i-x-mlp0",
    "href": "Notebook_04.html#update-function-phimathbfa_i-x-mlp0",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Update Function \\(\\phi^*(\\mathbf{a}_i, x)\\) (MLP0)",
    "text": "Update Function \\(\\phi^*(\\mathbf{a}_i, x)\\) (MLP0)\nLearned update functions after training. Each curve represents one neuron. Colors indicate true neuron types. True functions overlaid in gray.\n\n\n\n\n\nSupp. Fig 8g: Update functions \\(\\phi^*(a_i, x)\\) (5% sparsity). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (10% sparsity). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (20% sparsity). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (50% sparsity). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (100% connectivity). True functions are overlaid in light gray."
  },
  {
    "objectID": "Notebook_04.html#transfer-function-psix-mlp1",
    "href": "Notebook_04.html#transfer-function-psix-mlp1",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Transfer Function \\(\\psi^*(x)\\) (MLP1)",
    "text": "Transfer Function \\(\\psi^*(x)\\) (MLP1)\nLearned transfer function after training, normalized to max=1. True function overlaid in gray.\n\n\n\n\n\nSupp. Fig 8h: Transfer function \\(\\psi^*(x)\\) (5% sparsity). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (10% sparsity). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (20% sparsity). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (50% sparsity). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (100% connectivity). True function overlaid in light gray."
  },
  {
    "objectID": "Notebook_04.html#latent-embeddings-mathbfa_i",
    "href": "Notebook_04.html#latent-embeddings-mathbfa_i",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "Latent Embeddings \\(\\mathbf{a}_i\\)",
    "text": "Latent Embeddings \\(\\mathbf{a}_i\\)\nLearned latent vectors for all neurons. Colors indicate true neuron types.\n\n\n\n\n\nSupp. Fig 8f: Latent embeddings \\(a_i\\) (5% sparsity).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (10% sparsity).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (20% sparsity).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (50% sparsity).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (100% connectivity)."
  },
  {
    "objectID": "Notebook_04.html#r²-connectivity-over-training-iterations",
    "href": "Notebook_04.html#r²-connectivity-over-training-iterations",
    "title": "Supplementary Figures 8 and 9: Sparse connectivity (5% to 100%)",
    "section": "R² Connectivity Over Training Iterations",
    "text": "R² Connectivity Over Training Iterations\n1000 densely connected neurons with 4 neuron-dependent update functions. The plot displays \\(R^2\\) for the comparison between true and learned connectivity matrices \\(W_{ij}\\) as a function of training iterations for different connectivity filling factors (colors). All comparisons are made at equal numbers of gradient descent iterations.\n\n\nCode\nprint()\nprint(\"-\" * 80)\nprint(\"Generating R² over iterations comparison plot\")\nprint(\"-\" * 80)\noutput_r2 = plot_r2_over_iterations(\n    config_list=config_list,\n    output_path='./log/signal/tmp_results/r2_over_iterations_sparsity.png',\n    device=device,\n)\n\n\n\n\n\n\n\n1000 densely connected neurons with 4 neuron-dependent update functions. \\(R^2\\) for the comparison between true and learned connectivity matrices \\(W_{ij}\\) as a function of training iterations for different connectivity filling factors (colors). All comparisons are made at equal numbers of gradient descent iterations."
  },
  {
    "objectID": "Notebook_02.html",
    "href": "Notebook_02.html",
    "title": "Supplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figures 3 and 4. To assess the importance of learning latent neuron types, we trained a GNN with fixed embedding. Models that ignore the heterogeneity of neural populations are poor approximations of the underlying dynamics\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j)\\]"
  },
  {
    "objectID": "Notebook_02.html#configuration-and-setup",
    "href": "Notebook_02.html#configuration-and-setup",
    "title": "Supplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 3: 1000 neurons, 4 types, dense connectivity, no embedding\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_3'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_02.html#step-1-generate-data",
    "href": "Notebook_02.html#step-1-generate-data",
    "title": "Supplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model. This creates the training dataset with 1000 neurons over 100,000 time points.\nOutputs:\n\nSample of 100 time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nSupp. Fig 3b: Sample of 100 time series taken from the activity data.\n\n\n\n\n\n\n\n\n\nSupp. Fig 3c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_02.html#step-2-train-gnn",
    "href": "Notebook_02.html#step-2-train-gnn",
    "title": "Supplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\) and functions \\(\\phi^*/\\psi^*\\) (without latent embeddings). The GNN learns to predict \\(dx/dt\\) from the observed activity \\(x\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\nwhere \\(\\phi^*\\) and \\(\\psi^*\\) are MLPs (ReLU, hidden dim=64, 3 layers) and \\(W\\) is the learnable connectivity matrix.\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, phi, psi (no embeddings)\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, functions phi* and psi* (no embeddings)\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_02.html#step-3-gnn-evaluation",
    "href": "Notebook_02.html#step-3-gnn-evaluation",
    "title": "Supplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 3 from the paper.\nFigure panels:\n\nSupp. Fig 3d: Learned connectivity matrix\nSupp. Fig 3e: Comparison of learned vs true connectivity\nSupp. Fig 3g: Learned update functions \\(\\phi^*(x)\\)\nSupp. Fig 3h: Learned transfer function \\(\\psi^*(x)\\)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 3 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"update functions phi*(x)\")\nprint(f\"transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nSupplementary Figure 3: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 3d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nSupp. Fig 3e: Comparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nSupp. Fig 3g: Learned update functions \\(\\phi^*(x)\\). True function is overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 3h: Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_02.html#step-4-test-model",
    "href": "Notebook_02.html#step-4-test-model",
    "title": "Supplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 4: Test Model",
    "text": "Step 4: Test Model\nTest the trained GNN model. Evaluates prediction accuracy and performs rollout inference.\n\n\nCode\n# STEP 4: TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 4: TEST - Evaluating trained model\")\nprint(\"-\" * 80)\nprint(f\"testing prediction accuracy and rollout inference\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nconfig.simulation.noise_model_level = 0.0\n\ndata_test(\n    config=config,\n    visualize=False,\n    style=\"color name continuous_slice\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=None,\n)\n\n\n\nRollout Results\nDisplay the rollout comparison figures showing: - Left panel: activity traces (ground truth gray, learned colored) - Right panel: scatter plot of true vs learned \\(x_i\\) with \\(R^2\\) and slope\n\n\n\n\n\nSupp. Fig 4ab: Rollout comparison up to time-point 400.\n\n\n\n\n\n\n\n\n\nSupp. Fig 4cd: Rollout comparison up to time-point 800."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Graph neural networks uncover structure and function underlying the activity of neural assemblies",
    "section": "",
    "text": "Graph neural networks trained to predict observable dynamics can be used to decompose the temporal activity of complex heterogeneous systems into simple, interpretable representations. Here we apply this framework to simulated neural assemblies with thousands of neurons and demonstrate that it can jointly reveal the connectivity matrix, the neuron types, the signaling functions, and in some cases hidden external stimuli. In contrast to existing machine learning approaches such as recurrent neural networks and transformers, which emphasize predictive accuracy but offer limited interpretability, our method provides both reliable forecasts of neural activity and interpretable decomposition of the mechanisms governing large neural assemblies.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFigure 2: baseline - 1000 neurons with 4 types\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_01.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figures 3 and 4: 1000 neurons with 4 types, training with fixed embedding\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_02.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 7: Effect of training dataset size\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_03.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figures 8 and 9: Sparse connectivity (5% to 100%)\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_04.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 10: effect of Gaussian noise\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nNoise\n\n\n\n\n\n\n\n\n\nNotebook_05.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 11: large scale - 8000 neurons\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nLarge Scale\n\n\n\n\n\n\n\n\n\nNotebook_06.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 12: many types - 32 neuron types\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nMany Types\n\n\n\n\n\n\n\n\n\nNotebook_07.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 13: heterogeneous transfer functions\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nTransmitters\n\n\n\n\n\n\n\n\n\nNotebook_08.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 14: neuron-dependent transfer functions\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nNeuron-dependent\n\n\n\n\n\n\n\n\n\nNotebook_09.py\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: External Inputs - 2048 neurons with external inputs\n\n\n\nNeural Activity\n\nExternal Inputs\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_10.py\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Notebook_01.html",
    "href": "Notebook_01.html",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "",
    "text": "This script reproduces the panels of paper’s Figure 2 and other related supplementary panels (Supp. 1, 2, 5 and 6).\nSimulation parameters:\nThe simulation follows a simplified Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j)\\]"
  },
  {
    "objectID": "Notebook_01.html#configuration-and-setup",
    "href": "Notebook_01.html#configuration-and-setup",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Figure 2: 1000 neurons, 4 types, dense connectivity\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_2'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_01.html#step-1-generate-data",
    "href": "Notebook_01.html#step-1-generate-data",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model (src/neural-gnn/generators). This creates the training dataset with 1000 neurons of 4 different types over 100,000 time points.\nOutputs:\n\nFigure 2b: Sample of 100 time series\nFigure 2c: True connectivity matrix \\(W_{ij}\\)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (Fig 2a-c)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nFig 2b: Sample of 100 time series taken from the activity data.\n\n\n\n\n\n\n\n\n\nFig 2c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_01.html#step-2-train-gnn",
    "href": "Notebook_01.html#step-2-train-gnn",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\), latent embeddings \\(\\mathbf{a}_i\\), and functions \\(\\phi^*, \\psi^*\\) with the SignalPropagation model (‘src/neural-gnn/models’). The GNN learns to predict \\(dx_i/dt\\) from the observed activity \\(x_i\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\nwhere \\(\\phi^*\\) and \\(\\psi^*\\) are MLPs (ReLU, hidden dim=64, 3 layers). \\(\\mathbf{a}_i\\) is a learnable 2D latent vector per neuron, and \\(W\\) is the learnable connectivity matrix.\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_01.html#step-3-gnn-evaluation",
    "href": "Notebook_01.html#step-3-gnn-evaluation",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Figure 2, and supplementary Fig 1, 2, 5, and 6 from the paper.\nFigure panels:\n\nFig 2d: Learned connectivity matrix\nFig 2e: Comparison of learned vs true connectivity\nFig 2f: Learned latent vectors \\(\\mathbf{a}_i\\)\nFig 2g: Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nFig 2h: Learned transfer function \\(\\psi^*(x)\\)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Figure 2 panels (d-h)\")\nprint(\"-\" * 80)\nprint(f\"Fig 2d: Learned connectivity matrix\")\nprint(f\"Fig 2e: W learned vs true (R^2, slope)\")\nprint(f\"Fig 2f: Latent vectors a_i (4 clusters)\")\nprint(f\"Fig 2g: Update functions phi*(a_i, x)\")\nprint(f\"Fig 2h: Transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nFigures 2d-2h: GNN Evaluation Results\n\n\n\n\n\nFig 2d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nFig 2e: Comparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nFig 2f: Learned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nFig 2g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves, one for each vector \\(a_i\\). Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nFig 2h: Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_01.html#step-4-gnn-training-visualization",
    "href": "Notebook_01.html#step-4-gnn-training-visualization",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 4: GNN Training Visualization",
    "text": "Step 4: GNN Training Visualization\nGenerate training progression figures showing how the GNN learns across epochs.\nVisualizations:\n\nRow a: Latent embeddings \\(\\mathbf{a}_i\\) evolution\nRow b: Update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nRow c: Transfer function \\(\\psi^*(x)\\)\nRow d: Connectivity matrix \\(W\\)\nRow e: \\(W\\) learned vs true scatter plot\n\n\n\nCode\n# STEP 4: GNN TRAINING VISUALIZATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 4: GNN TRAINING - Generating training progression figures\")\nprint(\"-\" * 80)\nprint(f\"generating plots for all training epochs\")\nprint(f\"output: {log_dir}/results/all/\")\nprint()\ndata_plot(config=config, config_file=config_file, epoch_list=['all'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n# Create montage from individual epoch plots\nprint()\nprint(\"creating training montage (8 columns x 5 rows)...\")\ncreate_training_montage(config=config, n_cols=8)\n\n\n\n\n\n\n\nSupplementary Figure 1: Results plotted over 20 epochs. (a) Learned latent vectors \\(a_i\\). (b) Learned update functions \\(\\phi^*(a_i, x)\\). (c) Learned transfer function \\(\\psi^*(x)\\), normalized to max=1. (d) Learned connectivity \\(W_{ij}\\). (e) Comparison of learned and true connectivity. Colors indicate true neuron types."
  },
  {
    "objectID": "Notebook_01.html#step-5-test-model",
    "href": "Notebook_01.html#step-5-test-model",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 5: Test Model",
    "text": "Step 5: Test Model\nTest the trained GNN model. Evaluates prediction accuracy and performs rollout inference.\n\n\nCode\n# STEP 5: TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 5: TEST - Evaluating trained model\")\nprint(\"-\" * 80)\nprint(f\"testing prediction accuracy and rollout inference\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nconfig.simulation.noise_model_level = 0.0\n\ndata_test(\n    config=config,\n    visualize=False,\n    style=\"color name continuous_slice\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=None,\n)\n\n\n\nRollout Results\n\nLeft panel: activity traces (ground truth gray, learned colored)\nRight panel: scatter plot of true vs learned \\(x_i\\) with \\(R^2\\) and slope\n\n\n\n\n\n\nRollout comparison up to time-point 400.\n\n\n\n\n\n\n\n\n\nRollout comparison up to time-point 800."
  },
  {
    "objectID": "Notebook_01.html#step-6-supplementary-figure-5---generalization-test",
    "href": "Notebook_01.html#step-6-supplementary-figure-5---generalization-test",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 6: Supplementary Figure 5 - Generalization Test",
    "text": "Step 6: Supplementary Figure 5 - Generalization Test\nTest the trained GNN with modified network structure. Modified neuron type proportions (10%, 20%, 30%, 40% instead of 25% each) and modified sparse connectivity (~25% sparsity, 243,831 weights instead of 10^6).\nOutputs:\n\nPanel b: Modified neuron type proportions histogram\nPanel d: Modified sparse connectivity matrix\nPanels e,f: Rollout at 400 time-points\nPanels g,h: Rollout at 800 time-points\n\n\n\nCode\n# STEP 6: SUPPLEMENTARY FIGURE 5 - GENERALIZATION TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 6: SUPPLEMENTARY FIGURE 5 - Generalization test with modified network\")\nprint(\"-\" * 80)\nprint(\"modified neuron type proportions: 10%, 20%, 30%, 40%\")\nprint(\"modified connectivity: ~25% sparsity (243,831 weights)\")\nprint()\n\n# new_params: [connectivity_filling_factor, type0_pct, type1_pct, type2_pct, type3_pct]\nnew_params_supp5 = [0.25, 10, 20, 30, 40]\n\ndata_test(\n    config=config,\n    visualize=True,\n    style=\"color\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=new_params_supp5,\n)\n\n\n\nSupplementary Figure 5 Panels\n\n\n\n\n\nPanel b: Modified neuron type proportions (10%, 20%, 30%, 40%).\n\n\n\n\n\n\n\n\n\nPanel d: Modified sparse connectivity matrix (~25% sparsity, 243,831 weights).\n\n\n\n\n\n\n\n\n\nPanels e,f: Rollout up to 400 time-points.\n\n\n\n\n\n\n\n\n\nPanels g,h: Rollout up to 800 time-points."
  },
  {
    "objectID": "Notebook_01.html#supplementary-figure-6---generalization-test",
    "href": "Notebook_01.html#supplementary-figure-6---generalization-test",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Supplementary Figure 6 - Generalization Test",
    "text": "Supplementary Figure 6 - Generalization Test\nTest the trained GNN with network modifications. Modified neuron type proportions: 60%, 40%, 0%, 0% (types 2 and 3 eliminated) and modified sparse connectivity: ~50% sparsity (487,401 weights instead of 10^6).\nOutputs:\n\nPanel b: Modified neuron type proportions histogram\nPanel d: Modified sparse connectivity matrix\nPanels e,f: Rollout at 400 time-points\nPanels g,h: Rollout at 800 time-points\n\n\n\nCode\n# SUPPLEMENTARY FIGURE 6 - GENERALIZATION TEST\nprint()\nprint(\"-\" * 80)\nprint(\"SUPPLEMENTARY FIGURE 6 - Generalization test with extreme network modification\")\nprint(\"-\" * 80)\nprint(\"modified neuron type proportions: 60%, 40%, 0%, 0% (types 2,3 eliminated)\")\nprint(\"modified connectivity: ~50% sparsity (487,401 weights)\")\nprint()\n\n# new_params: [connectivity_filling_factor, type0_pct, type1_pct, type2_pct, type3_pct]\n# 50% sparsity = 0.5 filling factor -&gt; ~500,000 weights\nnew_params_supp6 = [0.5, 60, 40, 0, 0]\n\ndata_test(\n    config=config,\n    visualize=True,\n    style=\"color\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=new_params_supp6,\n)\n\n\n\nSupplementary Figure 6 Panels\n\n\n\n\n\nPanel b: Modified neuron type proportions (60%, 40%, types 2,3 eliminated).\n\n\n\n\n\n\n\n\n\nPanel d: Modified sparse connectivity matrix (~50% sparsity, 487,401 weights).\n\n\n\n\n\n\n\n\n\nPanels e,f: Rollout up to 400 time-points.\n\n\n\n\n\n\n\n\n\nPanels g,h: Rollout up to 800 time-points."
  },
  {
    "objectID": "Notebook_03.html",
    "href": "Notebook_03.html",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "",
    "text": "This script reproduce the panels of paper’s Supplementary Figure 7. Performance scales with the length of the training series. This notebook displays connectivity matrix comparison, \\(\\phi^*\\) plots, \\(\\psi^*\\) plots, and learned embedding for each dataset size.\nSimulation parameters (constant across all experiments):\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j)\\]\nVariable: Training dataset size (n_frames)"
  },
  {
    "objectID": "Notebook_03.html#configuration",
    "href": "Notebook_03.html#configuration",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Configuration",
    "text": "Configuration\n\n\nCode\nimport glob\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 7: Effect of Training Dataset Size\")\nprint(\"=\" * 80)\n\n# All configs to process (config_name, n_frames)\nconfig_list = [\n    ('signal_fig_2', 100000),\n    ('signal_fig_supp_7_1', 50000),\n    ('signal_fig_supp_7_2', 40000),\n    ('signal_fig_supp_7_3', 30000),\n    ('signal_fig_supp_7_4', 20000),\n    ('signal_fig_supp_7_5', 10000),\n]\n\ndevice = []\nbest_model = ''\nconfig_root = \"./config\""
  },
  {
    "objectID": "Notebook_03.html#steps-1-3-generate-train-and-plot-for-all-configs",
    "href": "Notebook_03.html#steps-1-3-generate-train-and-plot-for-all-configs",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Steps 1-3: Generate, Train, and Plot for all configs",
    "text": "Steps 1-3: Generate, Train, and Plot for all configs\nLoop over all dataset sizes: generate data, train GNN, and generate plots. Skips steps if data/models already exist.\n\n\nCode\nfor config_file_, n_frames in config_list:\n    print()\n    print(\"=\" * 80)\n    print(f\"Processing: {config_file_} (n_frames={n_frames:,})\")\n    print(\"=\" * 80)\n\n    config_file, pre_folder = add_pre_folder(config_file_)\n\n    # Load config\n    config = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\n    config.config_file = config_file\n    config.dataset = config_file\n\n    if device == []:\n        device = set_device(config.training.device)\n\n    log_dir = f'./log/{config_file}'\n    graphs_dir = f'./graphs_data/{config_file}'\n\n    # STEP 1: GENERATE\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 1: GENERATE - Simulating neural activity\")\n    print(\"-\" * 80)\n\n    data_file = f'{graphs_dir}/x_list_0.npy'\n    if os.path.exists(data_file):\n        print(f\"data already exists at {graphs_dir}/\")\n        print(\"skipping simulation, regenerating figures...\")\n        data_generate(\n            config,\n            device=device,\n            visualize=False,\n            run_vizualized=0,\n            style=\"color\",\n            alpha=1,\n            erase=False,\n            bSave=True,\n            step=2,\n            regenerate_plots_only=True,\n        )\n    else:\n        print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_frames} frames\")\n        print(f\"output: {graphs_dir}/\")\n        data_generate(\n            config,\n            device=device,\n            visualize=False,\n            run_vizualized=0,\n            style=\"color\",\n            alpha=1,\n            erase=False,\n            bSave=True,\n            step=2,\n        )\n\n    # STEP 2: TRAIN\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 2: TRAIN - Training GNN\")\n    print(\"-\" * 80)\n\n    model_files = glob.glob(f'{log_dir}/models/*.pt')\n    if model_files:\n        print(f\"trained model already exists at {log_dir}/models/\")\n        print(\"skipping training (delete models folder to retrain)\")\n    else:\n        print(f\"training for {config.training.n_epochs} epochs\")\n        print(f\"n_frames: {config.simulation.n_frames}\")\n        data_train(\n            config=config,\n            erase=False,\n            best_model=best_model,\n            style='color',\n            device=device\n        )\n\n    # STEP 3: PLOT\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 3: PLOT - Generating figures\")\n    print(\"-\" * 80)\n\n    folder_name = f'{log_dir}/tmp_results/'\n    os.makedirs(folder_name, exist_ok=True)\n\n    data_plot(\n        config=config,\n        config_file=config_file,\n        epoch_list=['best'],\n        style='color',\n        extended='plots',\n        device=device,\n        apply_weight_correction=True,\n        plot_eigen_analysis=False\n    )\n\n    # STEP 4: TRAINING PROGRESSION (R² over iterations)\n    print()\n    print(\"-\" * 80)\n    print(\"STEP 4: TRAINING PROGRESSION - Computing R² over iterations\")\n    print(\"-\" * 80)\n\n    r2_file = f'{log_dir}/results/all/r2_over_iterations.json'\n    if os.path.exists(r2_file):\n        print(f\"R² data already exists at {r2_file}\")\n        print(\"skipping (delete results/all/ folder to recompute)\")\n    else:\n        data_plot(\n            config=config,\n            config_file=config_file,\n            epoch_list=['all'],\n            style='color',\n            extended='plots',\n            device=device,\n            apply_weight_correction=True,\n            plot_eigen_analysis=False,\n        )"
  },
  {
    "objectID": "Notebook_03.html#connectivity-matrix-comparison",
    "href": "Notebook_03.html#connectivity-matrix-comparison",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Connectivity Matrix Comparison",
    "text": "Connectivity Matrix Comparison\nLearned vs true connectivity matrix \\(W_{ij}\\) after training. The scatter plot shows \\(R^2\\) and slope metrics.\n\n\n\n\n\nFig 2e: Connectivity comparison (n_frames=100,000)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (n_frames=50,000)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (n_frames=40,000)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (n_frames=30,000)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (n_frames=20,000)\n\n\n\n\n\n\n\n\n\nConnectivity comparison (n_frames=10,000)"
  },
  {
    "objectID": "Notebook_03.html#update-function-phimathbfa_i-x-mlp0",
    "href": "Notebook_03.html#update-function-phimathbfa_i-x-mlp0",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Update Function \\(\\phi^*(\\mathbf{a}_i, x)\\) (MLP0)",
    "text": "Update Function \\(\\phi^*(\\mathbf{a}_i, x)\\) (MLP0)\nLearned update functions after training. Each curve represents one neuron. Colors indicate true neuron types. True functions overlaid in gray.\n\n\n\n\n\nFig 2g: Update functions \\(\\phi^*(a_i, x)\\) (n_frames=100,000). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (n_frames=50,000). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (n_frames=40,000). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (n_frames=30,000). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (n_frames=20,000). True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\) (n_frames=10,000). True functions are overlaid in light gray."
  },
  {
    "objectID": "Notebook_03.html#transfer-function-psix-mlp1",
    "href": "Notebook_03.html#transfer-function-psix-mlp1",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Transfer Function \\(\\psi^*(x)\\) (MLP1)",
    "text": "Transfer Function \\(\\psi^*(x)\\) (MLP1)\nLearned transfer function after training, normalized to max=1. True function overlaid in gray.\n\n\n\n\n\nFig 2h: Transfer function \\(\\psi^*(x)\\) (n_frames=100,000). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (n_frames=50,000). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (n_frames=40,000). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (n_frames=30,000). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (n_frames=20,000). True function overlaid in light gray.\n\n\n\n\n\n\n\n\n\nTransfer function \\(\\psi^*(x)\\) (n_frames=10,000). True function overlaid in light gray."
  },
  {
    "objectID": "Notebook_03.html#latent-embeddings-mathbfa_i",
    "href": "Notebook_03.html#latent-embeddings-mathbfa_i",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Latent Embeddings \\(\\mathbf{a}_i\\)",
    "text": "Latent Embeddings \\(\\mathbf{a}_i\\)\nLearned latent vectors for all neurons. Colors indicate true neuron types.\n\n\n\n\n\nFig 2f: Latent embeddings \\(a_i\\) (n_frames=100,000).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (n_frames=50,000).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (n_frames=40,000).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (n_frames=30,000).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (n_frames=20,000).\n\n\n\n\n\n\n\n\n\nLatent embeddings \\(a_i\\) (n_frames=10,000)."
  },
  {
    "objectID": "Notebook_03.html#r²-connectivity-over-training-iterations",
    "href": "Notebook_03.html#r²-connectivity-over-training-iterations",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "R² Connectivity Over Training Iterations",
    "text": "R² Connectivity Over Training Iterations\n\\(R^2\\) between learned and true connectivity \\(W_{ij}\\) plotted as a function of training iterations for each dataset size.\n\n\nCode\nprint()\nprint(\"-\" * 80)\nprint(\"Generating R² over iterations comparison plot\")\nprint(\"-\" * 80)\noutput_r2 = plot_r2_over_iterations(\n    config_list=config_list,\n    output_path='./log/signal/tmp_results/r2_over_iterations.png',\n    device=device,\n)\n\n\n\n\n\n\n\n\\(Supp. Fig 7: R^2\\) connectivity over training iterations for different dataset sizes."
  },
  {
    "objectID": "Notebook_05.html",
    "href": "Notebook_05.html",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 10. Gaussian noise is injected into the simulated dynamics (SNR of ∼10 dB).\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j) + \\eta_i(t)\\]"
  },
  {
    "objectID": "Notebook_05.html#configuration-and-setup",
    "href": "Notebook_05.html#configuration-and-setup",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 10: 1000 neurons, 4 types, dense connectivity, Gaussian noise\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_10'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_05.html#step-1-generate-data",
    "href": "Notebook_05.html#step-1-generate-data",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data with Gaussian noise using the PDE_N2 model. This creates the training dataset with 1000 neurons of 4 different types over 100,000 time points.\nOutputs:\n\nSupp. Fig 10b: Activity time series used for GNN training\nSupp. Fig 10c: True connectivity matrix \\(W_{ij}\\)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity with Gaussian noise\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames with Gaussian noise\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nSupp. Fig 10b: Sample of 100 time series taken from the activity data with Gaussian noise (~10 dB SNR).\n\n\n\n\n\n\n\n\n\nSupp. Fig 10c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_05.html#step-2-train-gnn",
    "href": "Notebook_05.html#step-2-train-gnn",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\), latent embeddings \\(\\mathbf{a}_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN learns to predict \\(dx_i/dt\\) from the noisy observed activity \\(x_i\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\nwhere \\(\\phi^*\\) and \\(\\psi^*\\) are MLPs (ReLU, hidden dim=64, 3 layers). \\(\\mathbf{a}_i\\) is a learnable 2D latent vector per neuron, and \\(W\\) is the learnable connectivity matrix.\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi from noisy data\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_05.html#step-3-gnn-evaluation",
    "href": "Notebook_05.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 10 from the paper.\nFigure panels:\n\nSupp. Fig 10d: Learned connectivity matrix\nSupp. Fig 10e: Comparison of learned vs true connectivity\nSupp. Fig 10f: Learned latent vectors \\(\\mathbf{a}_i\\)\nSupp. Fig 10g: Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nSupp. Fig 10h: Learned transfer function \\(\\psi^*(x)\\)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 10 panels (d-h)\")\nprint(\"-\" * 80)\nprint(f\"panel (d): Learned connectivity matrix\")\nprint(f\"panel (e): W learned vs true (R^2, slope)\")\nprint(f\"panel (f): Latent vectors a_i (4 clusters)\")\nprint(f\"panel (g): Update functions phi*(a_i, x)\")\nprint(f\"panel (h): Transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nSupplementary Figure 10: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 10d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nSupp. Fig 10e: Comparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nSupp. Fig 10f: Learned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nSupp. Fig 10g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves, one for each vector \\(a_i\\). Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 10h: Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_07.html",
    "href": "Notebook_07.html",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 12. Test with 32 different neuron types (update functions).\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\tanh(x_j)\\]"
  },
  {
    "objectID": "Notebook_07.html#configuration-and-setup",
    "href": "Notebook_07.html#configuration-and-setup",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 12: 1000 neurons, 32 types, dense connectivity\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_12'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_07.html#step-1-generate-data",
    "href": "Notebook_07.html#step-1-generate-data",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model with 32 neuron types. This tests the GNN’s ability to learn many distinct update functions.\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (32 neuron types)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nSupp. Fig 12b: Sample time series taken from the activity data (32 neuron types).\n\n\n\n\n\n\n\n\n\nSupp. Fig 12c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_07.html#step-2-train-gnn",
    "href": "Notebook_07.html#step-2-train-gnn",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\), latent embeddings \\(\\mathbf{a}_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN must learn to distinguish 32 different update functions.\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn 32 neuron types\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i for 32 types, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_07.html#step-3-gnn-evaluation",
    "href": "Notebook_07.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 12 from the paper.\nFigure panels:\n\nSupp. Fig 12d: Learned connectivity matrix\nSupp. Fig 12e: Comparison of learned vs true connectivity\nSupp. Fig 12f: Learned latent vectors \\(\\mathbf{a}_i\\) (32 clusters expected)\nSupp. Fig 12g: Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\) (32 distinct functions)\nSupp. Fig 12h: Learned transfer function \\(\\psi^*(x)\\)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 12 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (32 clusters)\")\nprint(f\"update functions phi*(a_i, x) - 32 distinct functions\")\nprint(f\"transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nSupplementary Figure 12: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 12d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nSupp. Fig 12e: Comparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nSupp. Fig 12f: Learned latent vectors \\(a_i\\) of all neurons. 32 clusters expected (one per neuron type).\n\n\n\n\n\n\n\n\n\nSupp. Fig 12g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves representing 32 distinct update functions. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 12h: Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_09.html",
    "href": "Notebook_09.html",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 14. Training with neuron-neuron dependent transfer functions of the form \\(\\psi(a_i, a_j, x_j)\\).\nSimulation parameters:\nThe simulation follows an extended version of Equation 2:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi_{ij}(x_j)\\]\nwhere the transfer function depends on both sender \\(j\\) and receiver \\(i\\):\n\\[\\psi_{ij}(x_j) = \\tanh\\left(\\frac{x_j}{\\gamma_i}\\right) - \\theta_j \\cdot x_j\\]\nThe GNN jointly optimizes the shared MLP \\(\\psi^*\\) and latent vectors \\(a_i\\) to accurately identify the neuron-neuron dependent transfer functions:\n\\[\\hat{\\dot{x}}_i = \\phi^*(a_i, x_i) + \\sum_j W_{ij} \\cdot \\psi^*(a_i, a_j, x_j)\\]"
  },
  {
    "objectID": "Notebook_09.html#configuration-and-setup",
    "href": "Notebook_09.html#configuration-and-setup",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\n\nCode\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 14: 1000 neurons, 4 types, neuron-dependent transfer functions\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_14'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_09.html#step-1-generate-data",
    "href": "Notebook_09.html#step-1-generate-data",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N5 model with neuron-dependent transfer functions. Each pair of neuron types has different transfer function characteristics depending on both source (\\(a_j\\)) and target (\\(a_i\\)) embeddings.\nOutputs:\n\nSupp. Fig 14b: Sample time series\nSupp. Fig 14c: True connectivity matrix \\(W_{ij}\\)\n\n\n\nCode\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (neuron-dependent transfer functions)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"transfer function gamma_i = [1, 2, 4, 8]\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\n\nSupp. Fig 14b: Sample time series taken from the activity data (neuron-dependent transfer functions).\n\n\n\n\n\n\n\n\n\nSupp. Fig 14c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_09.html#step-2-train-gnn",
    "href": "Notebook_09.html#step-2-train-gnn",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity \\(W\\), latent embeddings \\(\\mathbf{a}_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN must learn neuron-neuron dependent transfer functions \\(\\psi^*(\\mathbf{a}_i, \\mathbf{a}_j, x_j)\\).\n\n\nCode\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn neuron-dependent transfer functions\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, neuron-dependent psi*(a_i, a_j, x_j)\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_09.html#step-3-gnn-evaluation",
    "href": "Notebook_09.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 14 from the paper.\nFigure panels:\n\nSupp. Fig 14d: Learned connectivity\nSupp. Fig 14e: Comparison between learned and true connectivity\nSupp. Fig 14f: Learned latent vectors \\(a_i\\)\nSupp. Fig 14g: Learned update functions \\(\\phi^*(\\mathbf{a}, x)\\)\nSupp. Fig 14h: Learned transfer functions \\(\\psi^*(a_i, a_j, x)\\) (colors indicate true neuron types, true functions overlaid in light gray)\n\n\n\nCode\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 14 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (4 clusters)\")\nprint(f\"update functions phi*(a_i, x)\")\nprint(f\"transfer functions psi*(a_i, a_j, x) - neuron-neuron dependent\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True, plot_eigen_analysis=False)\n\n\n\nSupplementary Figure 14: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 14d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nSupp. Fig 14e: Comparison of learned and true connectivity (given \\(g_i\\)=10). Expected: \\(R^2\\)=0.99, slope=0.99.\n\n\n\n\n\n\n\n\n\nSupp. Fig 14f: Learned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nSupp. Fig 14g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 14h: Learned transfer functions \\(\\psi^*(a_i, a_j, x_j)\\). 2x2 montage: each panel corresponds to a receiving neuron type (border color), showing curves for all sending neuron types (line colors). True functions in gray."
  }
]