[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Graph neural networks uncover structure and function underlying the activity of neural assemblies",
    "section": "",
    "text": "Graph neural networks trained to predict observable dynamics can be used to decompose the temporal activity of complex heterogeneous systems into simple, interpretable representations. Here we apply this framework to simulated neural assemblies with thousands of neurons and demonstrate that it can jointly reveal the connectivity matrix, the neuron types, the signaling functions, and in some cases hidden external stimuli. In contrast to existing machine learning approaches such as recurrent neural networks and transformers, which emphasize predictive accuracy but offer limited interpretability, our method provides both reliable forecasts of neural activity and interpretable decomposition of the mechanisms governing large neural assemblies.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nFigure 2: baseline - 1000 neurons with 4 types\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_01.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_02.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 7: Effect of training dataset size\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_03.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 8: Sparse connectivity (5% to 100%)\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_04.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 10: effect of Gaussian noise\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nNoise\n\n\n\n\n\n\n\n\n\nNotebook_05.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 11: large scale - 8000 neurons\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nLarge Scale\n\n\n\n\n\n\n\n\n\nNotebook_06.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 12: many types - 32 neuron types\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nMany Types\n\n\n\n\n\n\n\n\n\nNotebook_07.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 13: heterogeneous transfer functions\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nTransmitters\n\n\n\n\n\n\n\n\n\nNotebook_08.py\n\n\n\n\n\n\n\n\n\n\n\n\nSupplementary Figure 14: neuron-dependent transfer functions\n\n\n\nNeural Activity\n\nSimulation\n\nGNN Training\n\nNeuron-dependent\n\n\n\n\n\n\n\n\n\nNotebook_09.py\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: External Inputs - 2048 neurons with Omega(t)\n\n\n\nNeural Activity\n\nExternal Inputs\n\nGNN Training\n\n\n\n\n\n\n\n\n\nNotebook_10.py\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Notebook_09.html",
    "href": "Notebook_09.html",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 14. Training with neuron-neuron dependent transfer functions (transmitters & receptors) of the form \\(\\psi(\\mathbf{a}_i, \\mathbf{a}_j, x_j)\\).\nSimulation parameters:\nThe simulation follows an extended version of Equation 2:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi_{ij}(x_j)\\]\nwhere the transfer function depends on both sender \\(j\\) and receiver \\(i\\):\n\\[\\psi_{ij}(x_j) = \\tanh\\left(\\frac{x_j}{\\gamma_i}\\right) - \\theta_j \\cdot x_j\\]\nThe GNN jointly optimizes the shared MLP \\(\\psi^*\\) and latent vectors \\(\\mathbf{a}_i\\) to accurately identify the neuron-neuron dependent transfer functions:\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\cdot \\psi^*(\\mathbf{a}_i, \\mathbf{a}_j, x_j)\\]"
  },
  {
    "objectID": "Notebook_09.html#configuration-and-setup",
    "href": "Notebook_09.html#configuration-and-setup",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 14: 1000 neurons, 4 types, neuron-dependent transfer functions\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_14'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_09.html#step-1-generate-data",
    "href": "Notebook_09.html#step-1-generate-data",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N5 model with neuron-dependent transfer functions. Each pair of neuron types has different transfer function characteristics depending on both source (\\(\\mathbf{a}_j\\)) and target (\\(\\mathbf{a}_i\\)) embeddings.\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (neuron-dependent transfer functions)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"transfer function widths gamma_i = [1, 2, 4, 8]\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nSample time series taken from the activity data (neuron-dependent transfer functions).\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_09.html#step-2-train-gnn",
    "href": "Notebook_09.html#step-2-train-gnn",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings \\(a_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN must learn neuron-neuron dependent transfer functions \\(\\psi^*(a_i, a_j, x_j)\\).\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn neuron-dependent transfer functions\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, neuron-dependent psi*(a_i, a_j, x_j)\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_09.html#step-3-gnn-evaluation",
    "href": "Notebook_09.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 14: neuron-dependent transfer functions",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 14 from the paper.\nFigure panels:\n\n\nActivity time series used for GNN training (10^5 time-points)\n\n\nSample of 10 time series taken from (a)\n\n\nTrue connectivity \\(W_{ij}\\)\n\n\nLearned connectivity\n\n\nComparison between learned and true connectivity\n\n\nLearned latent vectors \\(\\mathbf{a}_i\\)\n\n\nLearned update functions \\(\\phi^*(\\mathbf{a}, x)\\)\n\n\nLearned transfer functions \\(\\psi^*(\\mathbf{a}_i, \\mathbf{a}_j, x)\\) (colors indicate true neuron types, true functions overlaid in light gray)\n\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 14 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (4 clusters)\")\nprint(f\"update functions phi*(a_i, x)\")\nprint(f\"transfer functions psi*(a_i, a_j, x) - neuron-neuron dependent\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nSupplementary Figure 14: GNN Evaluation Results\n\n\n\n\n\nLearned connectivity.\n\n\n\n\n\n\n\n\n\nComparison of learned and true connectivity (given \\(g_i\\)=10). Expected: \\(R^2\\)=0.99, slope=0.99.\n\n\n\n\n\n\n\n\n\nLearned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nLearned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nLearned transfer functions \\(\\psi^*(a_i, a_j, x_j)\\). 2x2 montage: each panel corresponds to a receiving neuron type (border color), showing curves for all sending neuron types (line colors). True functions in gray."
  },
  {
    "objectID": "Notebook_07.html",
    "href": "Notebook_07.html",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 12. Test with 32 different neuron types (update functions).\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi(x_j)\\]\nClassification accuracy expected: 0.99"
  },
  {
    "objectID": "Notebook_07.html#configuration-and-setup",
    "href": "Notebook_07.html#configuration-and-setup",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 12: 1000 neurons, 32 types, dense connectivity\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_12'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_07.html#step-1-generate-data",
    "href": "Notebook_07.html#step-1-generate-data",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model with 32 neuron types. This tests the GNN’s ability to learn many distinct update functions.\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (32 neuron types)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nSample time series taken from the activity data (32 neuron types).\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_07.html#step-2-train-gnn",
    "href": "Notebook_07.html#step-2-train-gnn",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings \\(a_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN must learn to distinguish 32 different update functions.\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn 32 neuron types\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i for 32 types, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_07.html#step-3-gnn-evaluation",
    "href": "Notebook_07.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 12: many types - 32 neuron types",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 12 from the paper.\nFigure panels:\n\nLearned connectivity matrix\nComparison of learned vs true connectivity\nLearned latent vectors \\(\\mathbf{a}_i\\) (32 clusters expected)\nLearned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\) (32 distinct functions)\nLearned transfer function \\(\\psi^*(x)\\)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 12 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (32 clusters)\")\nprint(f\"update functions phi*(a_i, x) - 32 distinct functions\")\nprint(f\"transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nSupplementary Figure 12: GNN Evaluation Results\n\n\n\n\n\nLearned connectivity.\n\n\n\n\n\n\n\n\n\nComparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nLearned latent vectors \\(a_i\\) of all neurons. 32 clusters expected (one per neuron type).\n\n\n\n\n\n\n\n\n\nLearned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves representing 32 distinct update functions. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nLearned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_05.html",
    "href": "Notebook_05.html",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 10. Gaussian noise is injected into the simulated dynamics (SNR of ∼10 dB).\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi(x_j) + \\eta_i(t)\\]\nwhere \\(\\eta_i(t)\\) is Gaussian noise."
  },
  {
    "objectID": "Notebook_05.html#configuration-and-setup",
    "href": "Notebook_05.html#configuration-and-setup",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 10: 1000 neurons, 4 types, dense connectivity, Gaussian noise\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_10'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_05.html#step-1-generate-data",
    "href": "Notebook_05.html#step-1-generate-data",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data with Gaussian noise using the PDE_N2 model. This creates the training dataset with 1000 neurons of 4 different types and 100,000 time points.\nOutputs:\n\nPanel (a): Activity time series used for GNN training\nPanel (b): Sample of 10 time series\nPanel (c): True connectivity matrix \\(W_{ij}\\)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity with Gaussian noise\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames with Gaussian noise\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nPanel (b): Sample of 10 time series taken from the activity data with Gaussian noise (~10 dB SNR).\n\n\n\n\n\n\n\n\n\nPanel (c): True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_05.html#step-2-train-gnn",
    "href": "Notebook_05.html#step-2-train-gnn",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings \\(a_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN learns to predict \\(dx_i/dt\\) from the noisy observed activity \\(x_i\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\nwhere \\(\\phi^*\\) and \\(\\psi^*\\) are MLPs (ReLU, hidden dim=64, 3 layers). \\(\\mathbf{a}_i\\) is a learnable 2D latent vector per neuron, and \\(W\\) is the learnable connectivity matrix.\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi from noisy data\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_05.html#step-3-gnn-evaluation",
    "href": "Notebook_05.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 10: effect of Gaussian noise",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 10 from the paper.\nFigure panels:\n\nPanel (d): Learned connectivity matrix\nPanel (e): Comparison of learned vs true connectivity\nPanel (f): Learned latent vectors \\(\\mathbf{a}_i\\)\nPanel (g): Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nPanel (h): Learned transfer function \\(\\psi^*(x)\\)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 10 panels (d-h)\")\nprint(\"-\" * 80)\nprint(f\"panel (d): Learned connectivity matrix\")\nprint(f\"panel (e): W learned vs true (R^2, slope)\")\nprint(f\"panel (f): Latent vectors a_i (4 clusters)\")\nprint(f\"panel (g): Update functions phi*(a_i, x)\")\nprint(f\"panel (h): Transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nSupplementary Figure 10: GNN Evaluation Results\n\n\n\n\n\nPanel (d): Learned connectivity.\n\n\n\n\n\n\n\n\n\nPanel (e): Comparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nPanel (f): Learned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nPanel (g): Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves, one for each vector \\(a_i\\). Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nPanel (h): Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_03.html",
    "href": "Notebook_03.html",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "",
    "text": "This script reproduce the panels of paper’s Supplementary Figure 7. Performance scales with the length of the training series.\nSupplementary Figure 7 shows R² vs epochs for different training dataset sizes. This notebook displays connectivity matrix comparison and MLP0 plots for each dataset size.\nSimulation parameters (constant across all experiments):\nVariable: Training dataset size (n_frames)"
  },
  {
    "objectID": "Notebook_03.html#configuration",
    "href": "Notebook_03.html#configuration",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Configuration",
    "text": "Configuration\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 7: Effect of Training Dataset Size\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_7_5'  # 10,000 frames\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_03.html#step-1-generate-data",
    "href": "Notebook_03.html#step-1-generate-data",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data.\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity\")\nprint(\"-\" * 80)\n\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation...\")\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_frames} frames\")\n    print(f\"output: {graphs_dir}/\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )"
  },
  {
    "objectID": "Notebook_03.html#step-2-train-gnn",
    "href": "Notebook_03.html#step-2-train-gnn",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN.\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\n\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training...\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs\")\n    print(f\"n_frames: {config.simulation.n_frames}\")\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_03.html#step-3-generate-plots",
    "href": "Notebook_03.html#step-3-generate-plots",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Step 3: Generate Plots",
    "text": "Step 3: Generate Plots\nGenerate evaluation plots.\n\n# STEP 3: PLOT\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: PLOT - Generating figures\")\nprint(\"-\" * 80)\n\nfolder_name = f'{log_dir}/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\n\ndata_plot(\n    config=config,\n    config_file=config_file,\n    epoch_list=['best'],\n    style='color',\n    extended='plots',\n    device=device,\n    apply_weight_correction=True\n)"
  },
  {
    "objectID": "Notebook_03.html#connectivity-matrix-comparison-final",
    "href": "Notebook_03.html#connectivity-matrix-comparison-final",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Connectivity Matrix Comparison (Final)",
    "text": "Connectivity Matrix Comparison (Final)\nLearned vs true connectivity matrix \\(W_{ij}\\) after training. The scatter plot shows \\(R^2\\) and slope metrics.\n\n\n\n\n\nConnectivity comparison"
  },
  {
    "objectID": "Notebook_03.html#update-function-phimathbfa_i-x-mlp0---final",
    "href": "Notebook_03.html#update-function-phimathbfa_i-x-mlp0---final",
    "title": "Supplementary Figure 7: Effect of training dataset size",
    "section": "Update Function \\(\\phi^*(\\mathbf{a}_i, x)\\) (MLP0) - Final",
    "text": "Update Function \\(\\phi^*(\\mathbf{a}_i, x)\\) (MLP0) - Final\nLearned update functions after training. Each curve represents one neuron. Colors indicate true neuron types. True functions overlaid in gray.\n\n\n\n\n\nUpdate functions \\(\\phi^*(a_i, x)\\)"
  },
  {
    "objectID": "Notebook_01.html",
    "href": "Notebook_01.html",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "",
    "text": "This script reproduces the panels of paper’s Figure 2 and other related supplementary panels (1, 2, 5 and 6).\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi(x_j)\\]"
  },
  {
    "objectID": "Notebook_01.html#configuration-and-setup",
    "href": "Notebook_01.html#configuration-and-setup",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Figure 2: 1000 neurons, 4 types, dense connectivity\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_2'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_01.html#step-1-generate-data",
    "href": "Notebook_01.html#step-1-generate-data",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model (‘src/neural-gnn/generators’). This creates the training dataset with 1000 neurons of 4 different types and 100,000 time points.\nOutputs:\n\nFigure 2b: Sample of 100 time series\nFigure 2c: True connectivity matrix \\(W_{ij}\\)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (Fig 2a-c)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nFig 2b: Sample of 100 time series taken from the activity data.\n\n\n\n\n\n\n\n\n\nFig 2c: True connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_01.html#step-2-train-gnn",
    "href": "Notebook_01.html#step-2-train-gnn",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings a_i, and functions \\(\\phi^*, \\psi^*\\) with the SignalPropagation model (‘src/neural-gnn/models’). The GNN learns to predict \\(dx_i/dt\\) from the observed activity \\(x_i\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\nwhere \\(\\phi^*\\) and \\(\\psi^*\\) are MLPs (ReLU, hidden dim=64, 3 layers). \\(\\mathbf{a}_i\\) is a learnable 2D latent vector per neuron, and \\(W\\) is the learnable connectivity matrix.\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_01.html#step-3-gnn-evaluation",
    "href": "Notebook_01.html#step-3-gnn-evaluation",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Figure 2, and supplementary Fig 1, 2, 5, and 6 from the paper.\nFigure panels:\n\nFig 2d: Learned connectivity matrix\nFig 2e: Comparison of learned vs true connectivity\nFig 2f: Learned latent vectors \\(\\mathbf{a}_i\\)\nFig 2g: Learned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nFig 2h: Learned transfer function \\(\\psi^*(x)\\)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Figure 2 panels (d-h)\")\nprint(\"-\" * 80)\nprint(f\"Fig 2d: Learned connectivity matrix\")\nprint(f\"Fig 2e: W learned vs true (R^2, slope)\")\nprint(f\"Fig 2f: Latent vectors a_i (4 clusters)\")\nprint(f\"Fig 2g: Update functions phi*(a_i, x)\")\nprint(f\"Fig 2h: Transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nFigures 2d-2h: GNN Evaluation Results\n\n\n\n\n\nFig 2d: Learned connectivity.\n\n\n\n\n\n\n\n\n\nFig 2e: Comparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nFig 2f: Learned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nFig 2g: Learned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves, one for each vector \\(a_i\\). Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nFig 2h: Learned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_01.html#step-4-gnn-training-visualization",
    "href": "Notebook_01.html#step-4-gnn-training-visualization",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 4: GNN Training Visualization",
    "text": "Step 4: GNN Training Visualization\nGenerate training progression figures showing how the GNN learns across epochs.\nVisualizations:\n\nRow a: Latent embeddings \\(\\mathbf{a}_i\\) evolution\nRow b: Update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nRow c: Transfer function \\(\\psi^*(x)\\)\nRow d: Connectivity matrix \\(W\\)\nRow e: \\(W\\) learned vs true scatter plot\n\n\n# STEP 4: GNN TRAINING VISUALIZATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 4: GNN TRAINING - Generating training progression figures\")\nprint(\"-\" * 80)\nprint(f\"generating plots for all training epochs\")\nprint(f\"output: {log_dir}/results/all/\")\nprint()\ndata_plot(config=config, config_file=config_file, epoch_list=['all'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n# Create montage from individual epoch plots\nprint()\nprint(\"creating training montage (8 columns x 5 rows)...\")\ncreate_training_montage(config=config, n_cols=8)\n\n\n\n\n\n\nSupplementary Figure 1: Results plotted over 20 epochs. (a) Learned latent vectors \\(a_i\\). (b) Learned update functions \\(\\phi^*(a_i, x)\\). (c) Learned transfer function \\(\\psi^*(x)\\), normalized to max=1. (d) Learned connectivity \\(W_{ij}\\). (e) Comparison of learned and true connectivity. Colors indicate true neuron types."
  },
  {
    "objectID": "Notebook_01.html#step-5-test-model",
    "href": "Notebook_01.html#step-5-test-model",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 5: Test Model",
    "text": "Step 5: Test Model\nTest the trained GNN model. Evaluates prediction accuracy and performs rollout inference.\n\n# STEP 5: TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 5: TEST - Evaluating trained model\")\nprint(\"-\" * 80)\nprint(f\"testing prediction accuracy and rollout inference\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nconfig.simulation.noise_model_level = 0.0\n\ndata_test(\n    config=config,\n    visualize=False,\n    style=\"color name continuous_slice\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=None,\n)\n\n\nRollout Results\n\nLeft panel: activity traces (ground truth gray, learned colored)\nRight panel: scatter plot of true vs learned \\(x_i\\) with \\(R^2\\) and slope\n\n\n\n\n\n\nRollout comparison at time-point 400.\n\n\n\n\n\n\n\n\n\nRollout comparison at time-point 800."
  },
  {
    "objectID": "Notebook_01.html#step-6-supplementary-figure-5---generalization-test",
    "href": "Notebook_01.html#step-6-supplementary-figure-5---generalization-test",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Step 6: Supplementary Figure 5 - Generalization Test",
    "text": "Step 6: Supplementary Figure 5 - Generalization Test\nTest the trained GNN with modified network structure: - Modified neuron type proportions (10%, 20%, 30%, 40% instead of 25% each) - Modified sparse connectivity (~25% sparsity, 243,831 weights instead of 10^6)\nOutputs:\n\nPanel b: Modified neuron type proportions histogram\nPanel d: Modified sparse connectivity matrix\nPanels e,f: Rollout at 400 time-points\nPanels g,h: Rollout at 800 time-points\n\n\n# STEP 6: SUPPLEMENTARY FIGURE 5 - GENERALIZATION TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 6: SUPPLEMENTARY FIGURE 5 - Generalization test with modified network\")\nprint(\"-\" * 80)\nprint(\"modified neuron type proportions: 10%, 20%, 30%, 40%\")\nprint(\"modified connectivity: ~25% sparsity (243,831 weights)\")\nprint()\n\n# new_params: [connectivity_filling_factor, type0_pct, type1_pct, type2_pct, type3_pct]\nnew_params_supp5 = [0.25, 10, 20, 30, 40]\n\ndata_test(\n    config=config,\n    visualize=True,\n    style=\"color\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=new_params_supp5,\n)\n\n\nSupplementary Figure 5 Panels\n\n\n\n\n\nPanel b: Modified neuron type proportions (10%, 20%, 30%, 40%).\n\n\n\n\n\n\n\n\n\nPanel d: Modified sparse connectivity matrix (~25% sparsity, 243,831 weights).\n\n\n\n\n\n\n\n\n\nPanels e,f: Rollout at 400 time-points.\n\n\n\n\n\n\n\n\n\nPanels g,h: Rollout at 800 time-points."
  },
  {
    "objectID": "Notebook_01.html#supplementary-figure-6---generalization-test",
    "href": "Notebook_01.html#supplementary-figure-6---generalization-test",
    "title": "Figure 2: baseline - 1000 neurons with 4 types",
    "section": "Supplementary Figure 6 - Generalization Test",
    "text": "Supplementary Figure 6 - Generalization Test\nTest the trained GNN with network modifications: - Modified neuron type proportions: 60%, 40%, 0%, 0% (types 2 and 3 eliminated) - Modified sparse connectivity: ~50% sparsity (487,401 weights instead of 10^6)\nOutputs:\n\nPanel b: Modified neuron type proportions histogram\nPanel d: Modified sparse connectivity matrix\nPanels e,f: Rollout at 400 time-points\nPanels g,h: Rollout at 800 time-points\n\n\n# SUPPLEMENTARY FIGURE 6 - GENERALIZATION TEST\nprint()\nprint(\"-\" * 80)\nprint(\"SUPPLEMENTARY FIGURE 6 - Generalization test with extreme network modification\")\nprint(\"-\" * 80)\nprint(\"modified neuron type proportions: 60%, 40%, 0%, 0% (types 2,3 eliminated)\")\nprint(\"modified connectivity: ~50% sparsity (487,401 weights)\")\nprint()\n\n# new_params: [connectivity_filling_factor, type0_pct, type1_pct, type2_pct, type3_pct]\n# 50% sparsity = 0.5 filling factor -&gt; ~500,000 weights\nnew_params_supp6 = [0.5, 60, 40, 0, 0]\n\ndata_test(\n    config=config,\n    visualize=True,\n    style=\"color\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=new_params_supp6,\n)\n\n\nSupplementary Figure 6 Panels\n\n\n\n\n\nPanel b: Modified neuron type proportions (60%, 40%, types 2,3 eliminated).\n\n\n\n\n\n\n\n\n\nPanel d: Modified sparse connectivity matrix (~50% sparsity, 487,401 weights).\n\n\n\n\n\n\n\n\n\nPanels e,f: Rollout at 400 time-points.\n\n\n\n\n\n\n\n\n\nPanels g,h: Rollout at 800 time-points."
  },
  {
    "objectID": "Notebook_02.html",
    "href": "Notebook_02.html",
    "title": "Supplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 3. To assess the importance of learning latent neuron types, we trained a GNN with fixed embedding. Models that ignore the heterogeneity of neural populations are poor approximations of the underlying dynamics\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi(x_j)\\]"
  },
  {
    "objectID": "Notebook_02.html#configuration-and-setup",
    "href": "Notebook_02.html#configuration-and-setup",
    "title": "Supplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 3: 1000 neurons, 4 types, dense connectivity, no embedding\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_3'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_02.html#step-1-generate-data",
    "href": "Notebook_02.html#step-1-generate-data",
    "title": "Supplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model. This creates the training dataset with 1000 neurons and 100,000 time points.\nOutputs:\n\nSample of 10 time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nSample of 10 time series taken from the activity data.\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_02.html#step-2-train-gnn",
    "href": "Notebook_02.html#step-2-train-gnn",
    "title": "Supplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W and functions phi/psi (without latent embeddings). The GNN learns to predict dx/dt from the observed activity x.\nLearning targets:\n\nConnectivity matrix \\(W\\)\nUpdate function \\(\\phi^*(x)\\)\nTransfer function \\(\\psi^*(x)\\)\n\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, phi, psi (no embeddings)\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, functions phi* and psi* (no embeddings)\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_02.html#step-3-gnn-evaluation",
    "href": "Notebook_02.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 3 from the paper.\nFigure panels:\n\nLearned connectivity matrix\nComparison of learned vs true connectivity\nLearned update functions \\(\\phi^*(x)\\)\nLearned transfer function \\(\\psi^*(x)\\)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 3 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"update functions phi*(x)\")\nprint(f\"transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nSupplementary Figure 3: GNN Evaluation Results\n\n\n\n\n\nLearned connectivity.\n\n\n\n\n\n\n\n\n\nComparison of learned and true connectivity (given \\(g_i\\)=10).\n\n\n\n\n\n\n\n\n\nLearned update functions \\(\\phi^*(x)\\).\n\n\n\n\n\n\n\n\n\nLearned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_02.html#step-4-test-model",
    "href": "Notebook_02.html#step-4-test-model",
    "title": "Supplementary Figure 3: 1000 neurons with 4 types, training with fixed embedding",
    "section": "Step 4: Test Model",
    "text": "Step 4: Test Model\nTest the trained GNN model. Evaluates prediction accuracy and performs rollout inference.\n\n# STEP 4: TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 4: TEST - Evaluating trained model\")\nprint(\"-\" * 80)\nprint(f\"testing prediction accuracy and rollout inference\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nconfig.simulation.noise_model_level = 0.0\n\ndata_test(\n    config=config,\n    visualize=False,\n    style=\"color name continuous_slice\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=None,\n)\n\n\nRollout Results\nDisplay the rollout comparison figures showing: - Left panel: activity traces (ground truth gray, learned colored) - Right panel: scatter plot of true vs learned \\(x_i\\) with \\(R^2\\) and slope\n\n\n\n\n\nRollout comparison at time-point 400.\n\n\n\n\n\n\n\n\n\nRollout comparison at time-point 800."
  },
  {
    "objectID": "Notebook_04.html",
    "href": "Notebook_04.html",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 8. We tested GNN performance for connectivity matrices with varying sparsity levels: 5%, 10%, 20%, 50%, and 100% non-zero connections.\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi(x_j)\\]"
  },
  {
    "objectID": "Notebook_04.html#configuration-and-setup",
    "href": "Notebook_04.html#configuration-and-setup",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 8: 1000 neurons, 4 types, sparse connectivity 5%\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_8'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_04.html#step-1-generate-data",
    "href": "Notebook_04.html#step-1-generate-data",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model (‘src/neural-gnn/generators’). This creates the training dataset with 1000 neurons of 4 different types and 100,000 time points.\nOutputs:\n\nFigure 2b: Sample of 100 time series\nFigure 2c: True connectivity matrix W_ij\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (Fig 2a-c)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nSupp. Fig 8b: Sample of 100 time series taken from the activity data.\n\n\n\n\n\n\n\n\n\nSupp. Fig 8b: True connectivity W_ij. The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_04.html#step-2-train-gnn",
    "href": "Notebook_04.html#step-2-train-gnn",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings a_i, and functions \\(\\phi^*, \\psi^*\\) with the SignalPropagation model (‘src/neural-gnn/models’). The GNN learns to predict \\(dx_i/dt\\) from the observed activity \\(x_i\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\nwhere \\(\\phi^*\\) and \\(\\psi^*\\) are MLPs (ReLU, hidden dim=64, 3 layers). \\(\\mathbf{a}_i\\) is a learnable 2D latent vector per neuron, and \\(W\\) is the learnable connectivity matrix.\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_04.html#step-3-gnn-evaluation",
    "href": "Notebook_04.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Figure 2, and supplementary Fig 1, 2, 5, and 6 from the paper.\nFigure panels:\n\nFig 2d: Learned connectivity matrix\nFig 2e: Comparison of learned vs true connectivity\nFig 2f: Learned latent vectors a_i\nFig 2g: Learned update functions phi*(a_i, x)\nFig 2h: Learned transfer function psi*(x)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Figure 2 panels (d-h)\")\nprint(\"-\" * 80)\nprint(f\"Fig 2d: Learned connectivity matrix\")\nprint(f\"Fig 2e: W learned vs true (R^2, slope)\")\nprint(f\"Fig 2f: Latent vectors a_i (4 clusters)\")\nprint(f\"Fig 2g: Update functions phi*(a_i, x)\")\nprint(f\"Fig 2h: Transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nFigures 2d-2h: GNN Evaluation Results\n\n\n\n\n\nSupp. Fig 8c: Learned connectivity.\n\n\n\n\n\n\n\n\n\nSupp. Fig 8d: Comparison of learned and true connectivity (given g_i=10).\n\n\n\n\n\n\n\n\n\nSupp. Fig 8e: Learned latent vectors a_i of all neurons.\n\n\n\n\n\n\n\n\n\nSupp. Fig 8f: Learned update functions φ*(a_i, x). The plot shows 1000 overlaid curves, one for each vector a_i. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nSupp. Fig 8g: Learned transfer function ψ*(x), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_04.html#step-4-gnn-training-visualization",
    "href": "Notebook_04.html#step-4-gnn-training-visualization",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Step 4: GNN Training Visualization",
    "text": "Step 4: GNN Training Visualization\nGenerate training progression figures showing how the GNN learns across epochs.\nVisualizations:\n\nRow a: Latent embeddings a_i evolution\nRow b: Update functions phi*(a_i, x)\nRow c: Transfer function psi*(x)\nRow d: Connectivity matrix W\nRow e: W learned vs true scatter plot\n\n\n# STEP 4: GNN TRAINING VISUALIZATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 4: GNN TRAINING - Generating training progression figures\")\nprint(\"-\" * 80)\nprint(f\"generating plots for all training epochs\")\nprint(f\"output: {log_dir}/results/all/\")\nprint()\ndata_plot(config=config, config_file=config_file, epoch_list=['all'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n# Create montage from individual epoch plots\nprint()\nprint(\"creating training montage (8 columns x 5 rows)...\")\ncreate_training_montage(config=config, n_cols=8)\n\n\n\n\n\n\nResults plotted over 20 epochs. (a) Learned latent vectors a_i. (b) Learned update functions φ(a,x). (c) Learned transfer function ψ(x), normalized to max=1. (d) Learned connectivity W_ij. (e) Comparison of learned and true connectivity. Colors indicate true neuron types."
  },
  {
    "objectID": "Notebook_04.html#step-5-test-model",
    "href": "Notebook_04.html#step-5-test-model",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Step 5: Test Model",
    "text": "Step 5: Test Model\nTest the trained GNN model. Evaluates prediction accuracy and performs rollout inference.\n\n# STEP 5: TEST\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 5: TEST - Evaluating trained model\")\nprint(\"-\" * 80)\nprint(f\"testing prediction accuracy and rollout inference\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nconfig.simulation.noise_model_level = 0.0\n\ndata_test(\n    config=config,\n    visualize=True,\n    style=\"color name continuous_slice\",\n    verbose=False,\n    best_model='best',\n    run=0,\n    test_mode=\"\",\n    sample_embedding=False,\n    step=10,\n    n_rollout_frames=1000,\n    device=device,\n    particle_of_interest=0,\n    new_params=None,\n)\n\n\nRollout Results\nDisplay the rollout comparison figure showing: - Left panels: activity traces (ground truth gray, learned colored) - Top right: scatter plot of true vs learned \\(x_i\\) with \\(R^2\\) and slope - Bottom right: \\(R^2\\) over time"
  },
  {
    "objectID": "Notebook_04.html#sparsity-comparison",
    "href": "Notebook_04.html#sparsity-comparison",
    "title": "Supplementary Figure 8: Sparse connectivity (5% to 100%)",
    "section": "Sparsity Comparison",
    "text": "Sparsity Comparison\nComparison of GNN performance across different connectivity sparsity levels (5%, 10%, 20%, 50%, 100%). Each row shows the W learned vs true scatter plot (left) and learned update functions \\(\\phi^*(a_i, x)\\) (right).\n\n\n\n\n\nSparsity comparison: W learned vs true (left) and φ*(a_i, x) (right) for 5%, 10%, 20%, 50%, and 100% connectivity."
  },
  {
    "objectID": "Notebook_06.html",
    "href": "Notebook_06.html",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 11. Large scale test with 8,000 neurons and 64 million connectivity weights.\nSimulation parameters:\nThe simulation follows Equation 2 from the paper:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi(x_j) + \\eta_i(t)\\]"
  },
  {
    "objectID": "Notebook_06.html#configuration-and-setup",
    "href": "Notebook_06.html#configuration-and-setup",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 11: 8000 neurons, 4 types, dense connectivity (64M weights)\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_11'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_06.html#step-1-generate-data",
    "href": "Notebook_06.html#step-1-generate-data",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N2 model with 8000 neurons. This creates a large-scale training dataset with 64 million connectivity weights.\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\) (8000×8000 = 64M weights)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (8000 neurons)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"connectivity matrix: {config.simulation.n_neurons}x{config.simulation.n_neurons} = 64M weights\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nSample time series taken from the activity data (8000 neurons).\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\) (8000×8000 = 64 million weights). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_06.html#step-2-train-gnn",
    "href": "Notebook_06.html#step-2-train-gnn",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn the 64 million connectivity weights, latent embeddings \\(a_i\\), and functions \\(\\phi^*, \\psi^*\\).\nThe GNN optimizes the update rule (Equation 3 from the paper):\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(x_j)\\]\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn 64M connectivity weights\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: 64M connectivity weights, latent vectors a_i, functions phi* and psi*\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_06.html#step-3-gnn-evaluation",
    "href": "Notebook_06.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 11: large scale - 8000 neurons",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 11 from the paper.\nFigure panels:\n\nLearned connectivity matrix (64M weights)\nComparison of learned vs true connectivity\nLearned latent vectors \\(\\mathbf{a}_i\\)\nLearned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nLearned transfer function \\(\\psi^*(x)\\)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 11 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix (64M weights)\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (4 clusters)\")\nprint(f\"update functions phi*(a_i, x)\")\nprint(f\"transfer function psi*(x)\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nSupplementary Figure 11: GNN Evaluation Results\n\n\n\n\n\nLearned connectivity (8000×8000 = 64 million weights).\n\n\n\n\n\n\n\n\n\nComparison of learned and true connectivity (given \\(g_i\\)=10). Expected: \\(R^2\\)=1.00, slope=1.00.\n\n\n\n\n\n\n\n\n\nLearned latent vectors \\(a_i\\) of all 8000 neurons.\n\n\n\n\n\n\n\n\n\nLearned update functions \\(\\phi^*(a_i, x)\\). The plot shows 8000 overlaid curves. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nLearned transfer function \\(\\psi^*(x)\\), normalized to a maximum value of 1. True function is overlaid in light gray."
  },
  {
    "objectID": "Notebook_08.html",
    "href": "Notebook_08.html",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "",
    "text": "This script reproduces the panels of paper’s Supplementary Figure 13. Test with neuron-specific transfer functions (transmitters) of the form \\(\\psi(x_j/\\gamma_i)\\).\nSimulation parameters:\nThe simulation follows an extended version of Equation 2:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\cdot \\tanh(x_i) + g_i \\cdot \\sum_j W_{ij} \\cdot \\psi\\left(\\frac{x_j}{\\gamma_i}\\right)\\]\nThe GNN jointly optimizes the shared MLP \\(\\psi^*\\) and latent vectors \\(\\mathbf{a}_i\\) to accurately identify the neuron-specific transfer functions."
  },
  {
    "objectID": "Notebook_08.html#configuration-and-setup",
    "href": "Notebook_08.html#configuration-and-setup",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Supplementary Figure 13: 1000 neurons, 4 types, heterogeneous transfer functions\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_supp_13'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_08.html#step-1-generate-data",
    "href": "Notebook_08.html#step-1-generate-data",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N4 model with neuron-specific transfer functions. Each neuron type has a different width parameter \\(\\gamma_i\\) in the transfer function \\(\\psi(x_j/\\gamma_i)\\).\nOutputs:\n\nSample time series\nTrue connectivity matrix \\(W_{ij}\\)\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity (heterogeneous transfer functions)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"transfer function widths gamma_i = [1, 2, 4, 8]\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nSample time series taken from the activity data (heterogeneous transfer functions).\n\n\n\n\n\n\n\n\n\nTrue connectivity \\(W_{ij}\\). The inset shows 20×20 weights."
  },
  {
    "objectID": "Notebook_08.html#step-2-train-gnn",
    "href": "Notebook_08.html#step-2-train-gnn",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings \\(a_i\\), and functions \\(\\phi^*, \\psi^*\\). The GNN must learn neuron-specific transfer functions \\(\\psi(x_j/\\gamma_i)\\) with different widths.\nThe GNN optimizes the update rule:\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\sum_j W_{ij} \\psi^*(\\mathbf{a}_j, x_j)\\]\nwhere the transfer function \\(\\psi^*\\) now depends on the latent vector \\(\\mathbf{a}_j\\).\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn heterogeneous transfer functions\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, neuron-specific psi*(a_j, x_j)\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_08.html#step-3-gnn-evaluation",
    "href": "Notebook_08.html#step-3-gnn-evaluation",
    "title": "Supplementary Figure 13: heterogeneous transfer functions",
    "section": "Step 3: GNN Evaluation",
    "text": "Step 3: GNN Evaluation\nFigures matching Supplementary Figure 13 from the paper.\nFigure panels:\n\nLearned connectivity matrix\nComparison of learned vs true connectivity (expected: \\(R^2\\)=0.99, slope=0.99)\nLearned latent vectors \\(\\mathbf{a}_i\\)\nLearned update functions \\(\\phi^*(\\mathbf{a}_i, x)\\)\nLearned transfer functions \\(\\psi^*(\\mathbf{a}_j, x)\\) (4 different widths)\n\n\n# STEP 3: GNN EVALUATION\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: GNN EVALUATION - Generating Supplementary Figure 13 panels\")\nprint(\"-\" * 80)\nprint(f\"learned connectivity matrix\")\nprint(f\"W learned vs true (R^2, slope)\")\nprint(f\"latent vectors a_i (4 clusters)\")\nprint(f\"update functions phi*(a_i, x)\")\nprint(f\"transfer functions psi*(a_j, x) - 4 different widths\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)\n\n\nSupplementary Figure 13: GNN Evaluation Results\n\n\n\n\n\nLearned connectivity.\n\n\n\n\n\n\n\n\n\nComparison of learned and true connectivity (given \\(g_i\\)=10). Expected: \\(R^2\\)=0.99, slope=0.99.\n\n\n\n\n\n\n\n\n\nLearned latent vectors \\(a_i\\) of all neurons.\n\n\n\n\n\n\n\n\n\nLearned update functions \\(\\phi^*(a_i, x)\\). The plot shows 1000 overlaid curves. Colors indicate true neuron types. True functions are overlaid in light gray.\n\n\n\n\n\n\n\n\n\nLearned transfer functions \\(\\psi^*(a_j, x)\\) with 4 different widths (\\(\\gamma\\)=1,2,4,8), normalized to a maximum value of 1. True functions are overlaid in light gray."
  },
  {
    "objectID": "Notebook_10.html",
    "href": "Notebook_10.html",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "",
    "text": "This script reproduces Figure 3 from the paper: “Graph neural networks uncover structure and function underlying the activity of neural assemblies”\nSimulation parameters:\nThe simulation follows:\n\\[\\frac{dx_i}{dt} = -\\frac{x_i}{\\tau_i} + s_i \\tanh(x_i) + g_i \\Omega_i(t) \\sum_j W_{ij} \\left(\\tanh\\left(\\frac{x_j}{\\gamma_j}\\right) - \\theta_j x_j\\right) + \\eta_i(t)\\]\nThe GNN learns:\n\\[\\hat{\\dot{x}}_i = \\phi^*(\\mathbf{a}_i, x_i) + \\Omega_i^*(t) \\sum_j W_{ij} \\psi^*(\\mathbf{a}_i, \\mathbf{a}_j, x_j)\\]\nThe external input \\(\\Omega_i(t)\\) is a spatially-defined scalar field that modulates the connectivity for the first 1024 neurons. The remaining 1024 neurons have \\(\\Omega_i = 1\\)."
  },
  {
    "objectID": "Notebook_10.html#configuration-and-setup",
    "href": "Notebook_10.html#configuration-and-setup",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "Configuration and Setup",
    "text": "Configuration and Setup\n\nprint()\nprint(\"=\" * 80)\nprint(\"Figure 3: 2048 neurons, 4 types, with external inputs Omega(t)\")\nprint(\"=\" * 80)\n\ndevice = []\nbest_model = ''\nconfig_file_ = 'signal_fig_3'\n\nprint()\nconfig_root = \"./config\"\nconfig_file, pre_folder = add_pre_folder(config_file_)\n\n# load config\nconfig = NeuralGraphConfig.from_yaml(f\"{config_root}/{config_file}.yaml\")\nconfig.config_file = config_file\nconfig.dataset = config_file\n\nif device == []:\n    device = set_device(config.training.device)\n\nlog_dir = f'./log/{config_file}'\ngraphs_dir = f'./graphs_data/{config_file}'"
  },
  {
    "objectID": "Notebook_10.html#step-1-generate-data",
    "href": "Notebook_10.html#step-1-generate-data",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "Step 1: Generate Data",
    "text": "Step 1: Generate Data\nGenerate synthetic neural activity data using the PDE_N4 model. This creates the training dataset with 2048 neurons and external inputs.\nOutputs:\n\nFigure 3a: External inputs Omega_i(t) - time-dependent scalar field\nFigure 3b: Activity time series\nFigure 3c: Sample of 10 time series\n\n\n# STEP 1: GENERATE\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 1: GENERATE - Simulating neural activity with external inputs (Fig 3a-c)\")\nprint(\"-\" * 80)\n\n# Check if data already exists\ndata_file = f'{graphs_dir}/x_list_0.npy'\nif os.path.exists(data_file):\n    print(f\"data already exists at {graphs_dir}/\")\n    print(\"skipping simulation, regenerating figures...\")\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n        regenerate_plots_only=True,\n    )\nelse:\n    print(f\"simulating {config.simulation.n_neurons} neurons, {config.simulation.n_neuron_types} types\")\n    print(f\"external inputs: {config.simulation.n_input_neurons} neurons modulated by Omega(t)\")\n    print(f\"generating {config.simulation.n_frames} time frames\")\n    print(f\"output: {graphs_dir}/\")\n    print()\n    data_generate(\n        config,\n        device=device,\n        visualize=False,\n        run_vizualized=0,\n        style=\"color\",\n        alpha=1,\n        erase=False,\n        bSave=True,\n        step=2,\n    )\n\n\n\n\n\n\nFig 3a-b: (Top) External input field \\(\\Omega_i(t)\\) shown on a 32×32 grid (left, first 1024 neurons) and sunflower arrangement (right, remaining 1024 neurons). (Bottom) Neural activity \\(x_i\\) at time \\(t=0\\).\n\n\n\n\n\n\n\n\n\nFig 3c: Sample activity time series for 100 neurons over 10,000 time steps. Yellow dashed line shows the mean external input."
  },
  {
    "objectID": "Notebook_10.html#step-2-train-gnn",
    "href": "Notebook_10.html#step-2-train-gnn",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "Step 2: Train GNN",
    "text": "Step 2: Train GNN\nTrain the GNN to learn connectivity W, latent embeddings a_i, functions phi/psi, and the external input field Omega*(x, y, t) using a coordinate-based MLP (SIREN).\nLearning targets:\n\nConnectivity matrix W\nLatent vectors a_i\nUpdate function phi*(a_i, x)\nTransfer function psi*(x)\nExternal input field Omega*(x, y, t) via SIREN network\n\n\n# STEP 2: TRAIN\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 2: TRAIN - Training GNN to learn W, embeddings, phi, psi, and Omega*\")\nprint(\"-\" * 80)\n\n# Check if trained model already exists (any .pt file in models folder)\nimport glob\nmodel_files = glob.glob(f'{log_dir}/models/*.pt')\nif model_files:\n    print(f\"trained model already exists at {log_dir}/models/\")\n    print(\"skipping training (delete models folder to retrain)\")\nelse:\n    print(f\"training for {config.training.n_epochs} epochs, {config.training.n_runs} run(s)\")\n    print(f\"learning: connectivity W, latent vectors a_i, functions phi*, psi*\")\n    print(f\"learning: external input field Omega*(x, y, t) via SIREN network\")\n    print(f\"models: {log_dir}/models/\")\n    print(f\"training plots: {log_dir}/tmp_training\")\n    print(f\"tensorboard: tensorboard --logdir {log_dir}/\")\n    print()\n    data_train(\n        config=config,\n        erase=False,\n        best_model=best_model,\n        style='color',\n        device=device\n    )"
  },
  {
    "objectID": "Notebook_10.html#step-3-generate-publication-figures",
    "href": "Notebook_10.html#step-3-generate-publication-figures",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "Step 3: Generate Publication Figures",
    "text": "Step 3: Generate Publication Figures\nGenerate publication-quality figures matching Figure 3 from the paper.\nFigure panels:\n\nFig 3d: Comparison of learned vs true connectivity W_ij\nFig 3e: Comparison of learned vs true Omega_i(t) values\nFig 3f: True field Omega_i(t) at different time-points\nFig 3g: Learned field Omega*(t) at different time-points\n\n\n# STEP 3: PLOT\nprint()\nprint(\"-\" * 80)\nprint(\"STEP 3: PLOT - Generating Figure 3 panels (d-g)\")\nprint(\"-\" * 80)\nprint(f\"Fig 3d: W learned vs true (R^2, slope)\")\nprint(f\"Fig 3e: Omega learned vs true\")\nprint(f\"Fig 3f: True field Omega_i(t) at different times\")\nprint(f\"Fig 3g: Learned field Omega*(t) at different times\")\nprint(f\"output: {log_dir}/results/\")\nprint()\nfolder_name = './log/' + pre_folder + '/tmp_results/'\nos.makedirs(folder_name, exist_ok=True)\ndata_plot(config=config, config_file=config_file, epoch_list=['best'], style='color', extended='plots', device=device, apply_weight_correction=True)"
  },
  {
    "objectID": "Notebook_10.html#output-files",
    "href": "Notebook_10.html#output-files",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "Output Files",
    "text": "Output Files\nRename output files to match Figure 3 panels.\n\n# Rename output files to match Figure 3 panels\nprint()\nprint(\"-\" * 80)\nprint(\"renaming output files to Figure 3 panels\")\nprint(\"-\" * 80)\n\nresults_dir = f'{log_dir}/results'\nos.makedirs(results_dir, exist_ok=True)\n\n# File mapping for simple copies\nfile_mapping = {\n    f'{graphs_dir}/activity_sample.png': f'{results_dir}/Fig3d_activity_sample.png',\n    f'{results_dir}/weights_comparison_corrected.png': f'{results_dir}/Fig3e_weights_comparison.png',\n}\n\nfor src, dst in file_mapping.items():\n    if os.path.exists(src):\n        shutil.copy2(src, dst)\n        print(f\"{os.path.basename(dst)}\")\n\nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Copy Fig 3a-b from generated frame (plot_synaptic_frame_visual output)\nfig_file = f'{graphs_dir}/Fig/Fig_0_000000.png'\nif os.path.exists(fig_file):\n    shutil.copy2(fig_file, f'{results_dir}/Fig3ab_external_input_activity.png')\n    print(f\"Fig3ab_external_input_activity.png\")\n\n# Copy Fig 3c: Activity time series\nif os.path.exists(f'{graphs_dir}/activity.png'):\n    shutil.copy2(f'{graphs_dir}/activity.png', f'{results_dir}/Fig3c_activity_time_series.png')\n    print(f\"Fig3c_activity_time_series.png\")\n\n# Generate Fig 3f: True field Omega_i(t) montage from field images\nprint(\"generating Fig3f_omega_field_true.png (5-frame montage)...\")\nfield_dir = f'{results_dir}/field'\nframe_indices = [0, 10000, 20000, 30000, 40000]\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, frame in enumerate(frame_indices):\n    ax = axes[idx]\n    # Find true field image for this frame\n    true_field_files = sorted(glob.glob(f'{field_dir}/true_field*_{frame}.png'))\n    if true_field_files:\n        img = mpimg.imread(true_field_files[-1])\n        ax.imshow(img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f't={frame}', fontsize=12)\n    ax.axis('off')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/Fig3f_omega_field_true.png', dpi=150)\nplt.close()\nprint(f\"Fig3f_omega_field_true.png\")\n\n# Generate Fig 3g: Learned field Omega*(t) montage from field images\nprint(\"generating Fig3g_omega_field_learned.png (5-frame montage)...\")\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\nfor idx, frame in enumerate(frame_indices):\n    ax = axes[idx]\n    # Find learned field image for this frame\n    learned_field_files = sorted(glob.glob(f'{field_dir}/reconstructed_field_LR*_{frame}.png'))\n    if learned_field_files:\n        img = mpimg.imread(learned_field_files[-1])\n        ax.imshow(img, cmap='gray')\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(f't={frame}', fontsize=12)\n    ax.axis('off')\nplt.tight_layout()\nplt.savefig(f'{results_dir}/Fig3g_omega_field_learned.png', dpi=150)\nplt.close()\nprint(f\"Fig3g_omega_field_learned.png\")\n\nprint()\nprint(\"=\" * 80)\nprint(\"Figure 3 complete!\")\nprint(f\"results saved to: {log_dir}/results/\")\nprint(\"=\" * 80)"
  },
  {
    "objectID": "Notebook_10.html#figure-3-panels",
    "href": "Notebook_10.html#figure-3-panels",
    "title": "Figure 3: External Inputs - 2048 neurons with Omega(t)",
    "section": "Figure 3 Panels",
    "text": "Figure 3 Panels\n\n\n\n\n\nFig 3d: Comparison of learned and true connectivity.\n\n\n\n\n\n\n\n\n\nFig 3e: Comparison of learned and true \\(\\Omega_i\\) values.\n\n\n\n\n\nFig 3f-g: True and Learned External Input Fields\nShowing \\(\\Omega_i(t)\\) at frames 0, 10000, 20000, 30000, 40000.\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 0.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 0.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 10000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 10000.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 20000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 20000.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 30000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 30000.\n\n\n\n\n\n\n\n\n\nTrue field \\(\\Omega_i\\) at frame 40000.\n\n\n\n\n\n\n\n\n\nLearned field \\(\\Omega^*_i\\) at frame 40000."
  }
]